{
    "docs": [
        {
            "location": "/", 
            "text": "OSG Networking Area\n\n\nWelcome to OSG Networking !\n This is an entry point for those interested in Networking in OSG or for those OSG users experiencing network problems. It provides an overview of the networking goals, plans and various activities and subtopics underway regarding networking in the Open Science Grid (OSG) and World-wide LCG Computing Grid (WLCG), operated as a joint project. This area started in June 2012 with initial focus on the network monitoring as monitoring is critical to provide needed visibility into existing networks and site connectivity. OSG is working to provide needed networking information and tools for users, sites and experiments/VOs.\n\n\nThis documentation is divided into several sub-sections, each covering a specific area of activities. \n\n\nNetwork Monitoring in WLCG and OSG (perfSONAR)\n\n\nWLCG and OSG jointly operate a network of perfSONAR agents deployed world-wide, which provides an open platform that can be used to baseline network performance and debug any potential issues. The following subsections provide details on the motivation, deployment and operations of the perfSONARs in WLCG/OSG: \n\n\n\n\nMotivation\n\n\nDeployment Guide\n - deployment models and options, hardware requirements\n\n\nInstallation and Administration Guide\n - installation, configuration and maintanance \n\n\nFrequently Asked Questions\n\n\n\n\nNetwork Troubleshooting\n\n\nUsers with network issues should check the \ntroubleshooting link\n below for initial guidance on how best to get their issue resolved. In addition, you can refer to the \nESNet network performance guide\n for a detailed instructions on how to identify and isolate network performance issues using perfSONAR.\n\n\nNetwork Service\n\n\nOSG operates an advance platform to collect, store, publish and analyse the network monitoring data it gathers from perfSONAR and other locations. All measurements are collected and available via streaming or through APIs. The following services are available:\n\n\n\n\nperfSONAR infrastructure monitoring\n - collects data on existing perfSONAR network, monitors its state and reports on availability of core services\n\n\nOSG Network Datastore\n - central datastore holding all the network measurements and providing an API to expose them via JSON. Datastore is based on \nESMOND\n, which supports the following \nAPI\n and runs at this \nendpoint\n.\n\n\nOSG Network Stream\n - access to network measurements in near realtime is provided by the GOC RabbitMQ and CERN ActiveMQ messaging brokers.\n\n\nOSG Dashboards\n- set of dashboards showing an overview of the network state as seen by the perfSONAR infrastructure \n\n\nWLCG Dashboards\n- set of dashboards showing WLCG and OSG network performance by combining multiple sources of data including perfSONAR, FTS, ESNet/LHCOPN traffic, etc. \n\n\n\n\nNetwork Analytics\n\n\nTBA\n\n\nReferences\n\n\nThe \nperfSONAR\n toolkit is part of the \nperfSONAR\n project. The current perfSONAR-PS toolkit is \navailable for download\n. \n\n\nOSG supports a number of services for networking including a \ncentral datastore\n which hosts all the OSG/WLCG perfSONAR data.\n\n\n\n\nInformation on ESmond is available at \nhttp://software.es.net/esmond/\n\n\nInformation on querying the perfSONAR data from ESmond is at \nhttp://software.es.net/esmond/perfsonar_client_rest.html\n\n\nAccess to a JSON view of the OSG network datastore is available at \nhttp://psds.grid.iu.edu/esmond/perfsonar/archive/?format=json\n\n\n\n\nWe can monitor the OSG network metrics using \nMaDDash\n (ESnet's Monitoring and Diagnostic Dashboard)\n\n\n\n\nOSG \nMaDDash\n instance \nhttp://psmad.grid.iu.edu/maddash-webui/index.cgi\n\n\n\n\nThe \nbasic service checking\n to track and monitor all our perfSONAR services uses \nOMD/Check_mk\n. NOTE: You need an x.509 credential in your browser to view this\n\n\n\n\nOSG OMD/Check_MK\n service monitoring \nhttps://psetf.grid.iu.edu/etf/check_mk/\n\n\n\n\nIlija Vukotic/University of Chicago has setup an \nanalytics platform\n (\nhttps://twiki.cern.ch/twiki/bin/view/AtlasComputing/ATLASAnalytics\n) using \nElastic Search\n and \nKibana4\n.\n\n\n\n\nHere is the \nKibana4\n web interface \nhttp://atlas-kibana.mwt2.org:5601/app/kibana#/dashboard/Default?_g=()\n\n\n\n\nESnet maintains an excellent set of pages about networking, end-system tuning, tools and techniques at \nhttps://fasterdata.es.net/\n\n\nThe are two related projects for OSG Networking\n\n\n\n\nPuNDIT\n (an OSG Satellite project) focusing on analyzing perfSONAR data to alert on problems: \nhttp://pundit.gatech.edu/\n\n\nMadAlert\n which analyzes \nMaDDash\n meshes to identify network problems: \nhttp://madalert.aglt2.org/madalert/", 
            "title": "OSG Networking Area"
        }, 
        {
            "location": "/#osg-networking-area", 
            "text": "Welcome to OSG Networking !  This is an entry point for those interested in Networking in OSG or for those OSG users experiencing network problems. It provides an overview of the networking goals, plans and various activities and subtopics underway regarding networking in the Open Science Grid (OSG) and World-wide LCG Computing Grid (WLCG), operated as a joint project. This area started in June 2012 with initial focus on the network monitoring as monitoring is critical to provide needed visibility into existing networks and site connectivity. OSG is working to provide needed networking information and tools for users, sites and experiments/VOs.  This documentation is divided into several sub-sections, each covering a specific area of activities.", 
            "title": "OSG Networking Area"
        }, 
        {
            "location": "/#network-monitoring-in-wlcg-and-osg-perfsonar", 
            "text": "WLCG and OSG jointly operate a network of perfSONAR agents deployed world-wide, which provides an open platform that can be used to baseline network performance and debug any potential issues. The following subsections provide details on the motivation, deployment and operations of the perfSONARs in WLCG/OSG:    Motivation  Deployment Guide  - deployment models and options, hardware requirements  Installation and Administration Guide  - installation, configuration and maintanance   Frequently Asked Questions", 
            "title": "Network Monitoring in WLCG and OSG (perfSONAR)"
        }, 
        {
            "location": "/#network-troubleshooting", 
            "text": "Users with network issues should check the  troubleshooting link  below for initial guidance on how best to get their issue resolved. In addition, you can refer to the  ESNet network performance guide  for a detailed instructions on how to identify and isolate network performance issues using perfSONAR.", 
            "title": "Network Troubleshooting"
        }, 
        {
            "location": "/#network-service", 
            "text": "OSG operates an advance platform to collect, store, publish and analyse the network monitoring data it gathers from perfSONAR and other locations. All measurements are collected and available via streaming or through APIs. The following services are available:   perfSONAR infrastructure monitoring  - collects data on existing perfSONAR network, monitors its state and reports on availability of core services  OSG Network Datastore  - central datastore holding all the network measurements and providing an API to expose them via JSON. Datastore is based on  ESMOND , which supports the following  API  and runs at this  endpoint .  OSG Network Stream  - access to network measurements in near realtime is provided by the GOC RabbitMQ and CERN ActiveMQ messaging brokers.  OSG Dashboards - set of dashboards showing an overview of the network state as seen by the perfSONAR infrastructure   WLCG Dashboards - set of dashboards showing WLCG and OSG network performance by combining multiple sources of data including perfSONAR, FTS, ESNet/LHCOPN traffic, etc.", 
            "title": "Network Service"
        }, 
        {
            "location": "/#network-analytics", 
            "text": "TBA", 
            "title": "Network Analytics"
        }, 
        {
            "location": "/#references", 
            "text": "The  perfSONAR  toolkit is part of the  perfSONAR  project. The current perfSONAR-PS toolkit is  available for download .   OSG supports a number of services for networking including a  central datastore  which hosts all the OSG/WLCG perfSONAR data.   Information on ESmond is available at  http://software.es.net/esmond/  Information on querying the perfSONAR data from ESmond is at  http://software.es.net/esmond/perfsonar_client_rest.html  Access to a JSON view of the OSG network datastore is available at  http://psds.grid.iu.edu/esmond/perfsonar/archive/?format=json   We can monitor the OSG network metrics using  MaDDash  (ESnet's Monitoring and Diagnostic Dashboard)   OSG  MaDDash  instance  http://psmad.grid.iu.edu/maddash-webui/index.cgi   The  basic service checking  to track and monitor all our perfSONAR services uses  OMD/Check_mk . NOTE: You need an x.509 credential in your browser to view this   OSG OMD/Check_MK  service monitoring  https://psetf.grid.iu.edu/etf/check_mk/   Ilija Vukotic/University of Chicago has setup an  analytics platform  ( https://twiki.cern.ch/twiki/bin/view/AtlasComputing/ATLASAnalytics ) using  Elastic Search  and  Kibana4 .   Here is the  Kibana4  web interface  http://atlas-kibana.mwt2.org:5601/app/kibana#/dashboard/Default?_g=()   ESnet maintains an excellent set of pages about networking, end-system tuning, tools and techniques at  https://fasterdata.es.net/  The are two related projects for OSG Networking   PuNDIT  (an OSG Satellite project) focusing on analyzing perfSONAR data to alert on problems:  http://pundit.gatech.edu/  MadAlert  which analyzes  MaDDash  meshes to identify network problems:  http://madalert.aglt2.org/madalert/", 
            "title": "References"
        }, 
        {
            "location": "/perfsonar-in-osg/", 
            "text": "Overview of perfSONAR\n\n\n \nFor those not familiar with \nperfSONAR\n, this page provides a quick overview of what it is and why we recommend its deployment at OSG and WLCG sites.\n\n\nOSG is working to support the scientific networking needs of it's constituents and collaborators. To do this, we are recommending all sites deploy perfSONAR \nso we can measure, monitor and diagnose the OSG (and WLCG) networks.\n\n\nThe Challenge\n\n\nDistributed scientific computing relies upon networks to interconnect resources and make them usable for scientific workflows. This dependency upon the network means that issues in \nour networks can significantly impact the behavior of all the various cyber-infrastructure components that rely upon it. Compounding the problem is that networks, by their nature, are \ndistributed and typically involve many different \"owners\" and administrators. When a problem arises somewhere along a network path, it can be very difficult to identify and localize.\n\n\nThis was the context for the formation of the perfSONAR collaboration (see \nhttps://www.perfsonar.net/about/mission-statement\n). This collaboration is focused on developing and \ndeploying the \nperfSONAR\n software suite in support of network monitoring across the global set of research and education (R\nE) networks. The \nOpen Science Grid\n (\nOSG\n) has chosen to \nbase the core of its network monitoring framework on \nperfSONAR\n because of both the capabilities of the toolkit for measuring our networks and its global acceptance as the defacto \nnetwork monitoring infrastructure of first choice.\n\n\nThe \nhttps://www.perfsonar.net.about/what-is-perfsonar/\n provides a succinct summary: ``\nperfSONAR is a network measurement toolkit designed to provide federated coverage of paths, \nand help to establish end-to-end usage expectations. There are 1000s of perfSONAR instances deployed world wide, many of which are available for open testing of key measures of network \nperformance. This global infrastructure helps to identify and isolate problems as they happen, making the role of supporting network users easier for engineering teams, and increasing \nproductivity when utilizing network resources.\n''\n\n\nOSG Networking\n\n\nHow can OSG members and collaborators understand, maintain and effectively utilize the networks that form the basis of their distributed collaborations?\n\n\nOur answer starts by providing visibility into our networks by the deployment of \nperfSONAR\n. perfSONAR allows us to regularly and consistently measure a set of network metrics that we \ncan use to understand how our networks are operating. When problems arise, the data, along with access to the \nperfSONAR\n tools, can be used to diagnose and localize problems. The \npresence of perfSONAR toolkit deployments across our sites and networks makes identifying and fixing network problems feasible.\n\n\nWe strongly recommend that all OSG (and WLCG) sites deploy \nperfSONAR\n toolkit instances as described in our \ninstallation guide\n. Before installing you should consult \nthe \nrequirements\n along with the guidance on \ndeployment models\n. \n\n\n\n\nNote\n\n\nInstalling perfSONAR not only benefits users at a site but will enable network engineers and OSG staff to much more effectively support those sites if network issues are suspected.\n\n\n\n\nAll OSG and WLCG sites should deploy \ntwo\n \nperfSONAR\n instances: one to measure latency/packet loss and one to measure bandwidth. \nIt is possible to install both versions on a single host with at least two NICs by following the instructions at \nhttp://docs.perfsonar.net/manage_dual_xface.html\n but you should \nread the \nmultiple NIC guidance page\n. \n\n\n\n\nWarning\n\n\nIt is \nvery important\n that the perfSONAR instances be located in the same subnet as the primary storage for the site. \nThis is to ensure that we are measuring as much of the network path involved with data transfer as possible.\n\n\n\n\nOur \nstrong recommendation\n for anyone maintaining/using perfSONAR is that they join either/both of the following mailing lists:\n\n\n\n\nUser's Mailing List\n The perfSONAR project maintains a mailing list for communication on matters of installation, configuration, bug reports, or general performance discussions: \nhttps://lists.internet2.edu/sympa/subscribe/perfsonar-user\n\n\nAnnouncement Mailing List\n The perfSONAR project also maintains a low volume mailing list used for announcements related to software updates and vulnerabilities: \nhttps://lists.internet2.edu/sympa/subscribe/perfsonar-announce\n\n\n\n\nChanges for perfSONAR 4.0\n\n\nThe final release of perfSONAR 4.0 was put into the perfSONAR repository on April 17, 2017. All sites following our recommendation of having auto-updates enabled should have upgraded during within 1-2 days of that date. This release incorporates possibly the largest single change in perfSONAR\u2019s 15-year history, and the developers spent considerable time making sure it was right.\n\n\nHighlights include:\n\n\n\n\nNew scheduling software called pScheduler that provides increased visibility, extensibility and control in the measurement process.\n\n\nCentOS 7 support\n\n\nDebian perfsonar-toolkit bundle support\n\n\nUpdated graphs\n\n\nSupport for email alerting in MaDDash\n\n\n\n\nFor a more complete list of changes, see the full release notes at: \nhttp://www.perfsonar.net/release-notes/version-4-0/\n\n\nThe \nWLCG Network Throughput Working Group\n is responsible for monitoring the WLCG/OSG instances and for defining and maintaining the mesh-configurations that we use to control perfSONAR testing. Please contact them if you have questions or suggestions related to perfSONAR testing amongst WLCG sites.\n\n\n\n\n\n\n-- Main.ShawnMcKee - 12 Sep 2017", 
            "title": "Motivation"
        }, 
        {
            "location": "/perfsonar-in-osg/#overview-of-perfsonar", 
            "text": "For those not familiar with  perfSONAR , this page provides a quick overview of what it is and why we recommend its deployment at OSG and WLCG sites.  OSG is working to support the scientific networking needs of it's constituents and collaborators. To do this, we are recommending all sites deploy perfSONAR \nso we can measure, monitor and diagnose the OSG (and WLCG) networks.", 
            "title": "Overview of perfSONAR"
        }, 
        {
            "location": "/perfsonar-in-osg/#the-challenge", 
            "text": "Distributed scientific computing relies upon networks to interconnect resources and make them usable for scientific workflows. This dependency upon the network means that issues in \nour networks can significantly impact the behavior of all the various cyber-infrastructure components that rely upon it. Compounding the problem is that networks, by their nature, are \ndistributed and typically involve many different \"owners\" and administrators. When a problem arises somewhere along a network path, it can be very difficult to identify and localize.  This was the context for the formation of the perfSONAR collaboration (see  https://www.perfsonar.net/about/mission-statement ). This collaboration is focused on developing and \ndeploying the  perfSONAR  software suite in support of network monitoring across the global set of research and education (R E) networks. The  Open Science Grid  ( OSG ) has chosen to \nbase the core of its network monitoring framework on  perfSONAR  because of both the capabilities of the toolkit for measuring our networks and its global acceptance as the defacto \nnetwork monitoring infrastructure of first choice.  The  https://www.perfsonar.net.about/what-is-perfsonar/  provides a succinct summary: `` perfSONAR is a network measurement toolkit designed to provide federated coverage of paths, \nand help to establish end-to-end usage expectations. There are 1000s of perfSONAR instances deployed world wide, many of which are available for open testing of key measures of network \nperformance. This global infrastructure helps to identify and isolate problems as they happen, making the role of supporting network users easier for engineering teams, and increasing \nproductivity when utilizing network resources. ''", 
            "title": "The Challenge"
        }, 
        {
            "location": "/perfsonar-in-osg/#osg-networking", 
            "text": "How can OSG members and collaborators understand, maintain and effectively utilize the networks that form the basis of their distributed collaborations?  Our answer starts by providing visibility into our networks by the deployment of  perfSONAR . perfSONAR allows us to regularly and consistently measure a set of network metrics that we \ncan use to understand how our networks are operating. When problems arise, the data, along with access to the  perfSONAR  tools, can be used to diagnose and localize problems. The \npresence of perfSONAR toolkit deployments across our sites and networks makes identifying and fixing network problems feasible.  We strongly recommend that all OSG (and WLCG) sites deploy  perfSONAR  toolkit instances as described in our  installation guide . Before installing you should consult \nthe  requirements  along with the guidance on  deployment models .    Note  Installing perfSONAR not only benefits users at a site but will enable network engineers and OSG staff to much more effectively support those sites if network issues are suspected.   All OSG and WLCG sites should deploy  two   perfSONAR  instances: one to measure latency/packet loss and one to measure bandwidth. \nIt is possible to install both versions on a single host with at least two NICs by following the instructions at  http://docs.perfsonar.net/manage_dual_xface.html  but you should \nread the  multiple NIC guidance page .    Warning  It is  very important  that the perfSONAR instances be located in the same subnet as the primary storage for the site. \nThis is to ensure that we are measuring as much of the network path involved with data transfer as possible.   Our  strong recommendation  for anyone maintaining/using perfSONAR is that they join either/both of the following mailing lists:   User's Mailing List  The perfSONAR project maintains a mailing list for communication on matters of installation, configuration, bug reports, or general performance discussions:  https://lists.internet2.edu/sympa/subscribe/perfsonar-user  Announcement Mailing List  The perfSONAR project also maintains a low volume mailing list used for announcements related to software updates and vulnerabilities:  https://lists.internet2.edu/sympa/subscribe/perfsonar-announce", 
            "title": "OSG Networking"
        }, 
        {
            "location": "/perfsonar-in-osg/#changes-for-perfsonar-40", 
            "text": "The final release of perfSONAR 4.0 was put into the perfSONAR repository on April 17, 2017. All sites following our recommendation of having auto-updates enabled should have upgraded during within 1-2 days of that date. This release incorporates possibly the largest single change in perfSONAR\u2019s 15-year history, and the developers spent considerable time making sure it was right.  Highlights include:   New scheduling software called pScheduler that provides increased visibility, extensibility and control in the measurement process.  CentOS 7 support  Debian perfsonar-toolkit bundle support  Updated graphs  Support for email alerting in MaDDash   For a more complete list of changes, see the full release notes at:  http://www.perfsonar.net/release-notes/version-4-0/  The  WLCG Network Throughput Working Group  is responsible for monitoring the WLCG/OSG instances and for defining and maintaining the mesh-configurations that we use to control perfSONAR testing. Please contact them if you have questions or suggestions related to perfSONAR testing amongst WLCG sites.    -- Main.ShawnMcKee - 12 Sep 2017", 
            "title": "Changes for perfSONAR 4.0"
        }, 
        {
            "location": "/perfsonar/deployment-models/", 
            "text": "perfSONAR Deployment Models\n\n\nperfSONAR can be deployed in a number of different ways:\n\n\n\n\nFrom ISO\n\n\nUsing RPMS\n\n\nvia container\n\n\nvia virtual machine\n\n\n\n\nThis document discusses the various deployment models and their requirements, pros and cons.", 
            "title": "Deployment"
        }, 
        {
            "location": "/perfsonar/deployment-models/#perfsonar-deployment-models", 
            "text": "perfSONAR can be deployed in a number of different ways:   From ISO  Using RPMS  via container  via virtual machine   This document discusses the various deployment models and their requirements, pros and cons.", 
            "title": "perfSONAR Deployment Models"
        }, 
        {
            "location": "/perfsonar/installation/", 
            "text": "Installation and Upgrade Details\n\n\n\n\nThis page documents installing or upgrading \nperfSONAR\n for OSG and WLCG sites. In case this is the first time you're trying to install and integrate your perfSONAR into WLCG or OSG, please consult \nhttps://opensciencegrid.github.io/networking/perfsonar-in-osg/\n for an overview of perfSONAR in OSG.\n\n\nFor any questions or help with WLCG perfSONAR setup, please email \nwlcg-perfsonar-support 'at' cern.ch\n or open GGUS ticket for \nWLCG perfSONAR Monitoring Support Unit\n. We strongly recommend anyone maintaining/using perfSONAR to join \nhttps://lists.internet2.edu/sympa/subscribe/perfsonar-user\n and \nhttps://lists.internet2.edu/sympa/subscribe/perfsonar-announce\n\n\nPlease follow the instructions for \nupgrading\n or \ninstalling\n perfSONAR as appropriate for your plans.\n\n\nUpdating perfSONAR\n\n\nBefore updating, please read the release notes at \nhttp://docs.perfsonar.net/manage_update.html#special-upgrade-notes\n\n\nTo update please run the following:\n\n\n# yum clean all\n# yum update\n# reboot\n\n\n\n\n\nNote, it's \nvery important to reboot\n otherwise your update is incomplete. Please also note, that starting from v3.4 \nyum\n auto-updates are turned on by default. \nWe strongly recommend keeping them turned on\n. With \nyum\n auto-updates in place there is a possibility that updated packages can \"break\" your perfSONAR install but this is viewed an acceptable risk in order to have security updates quickly applied on perfSONAR instances.\n\n\nIf you haven't yet added another (\nnon-root\n) user to administer the web interface, you should run \n/usr/lib/perfsonar/scripts/nptoolkit-configure.py\n and do so, as suggested in \nmotd\n. See \nhttp://docs.perfsonar.net/manage_users.html\n for detailed instructions.\n\n\nPlease backup your old \n/etc/perfsonar/meshconfig-agent.conf\n file and replace it with \n/etc/perfsonar/meshconfig-agent.conf.rpmnew\n.\n\n\nThen please verify/reconfigure your boxes following \nOSG and WLCG Configuration\n. For any further questions, please consult \nTroubleshooting\n pages, perfSONAR documentation (\nhttp://docs.perfsonar.net\n) or contact directly WLCG or OSG perfSONAR support units.\n\n\nInstalling perfSONAR\n\n\nperfSONAR 4.0 was released in April 2017. For sites running the previous version (3.x.y) the update should have automatically happened assuming that yum auto-updates were enabled and the \n/etc/yum.repos.d/Internet2.repo\n had 'enabled = 1'.\n\n\nThe following information details how to install perfSONAR 4.0 for WLCG and OSG. For general troubleshooting please go \nhere\n, in case you have any questions please email \nwlcg-perfsonar-support 'at' cern.ch\n or open GGUS ticket for \nWLCG perfSONAR support unit\n. For OSG sites that don't participate in WLCG, please contact \nOSG operations\n.\n\n\nThe perfSONAR team has created very good documentation on the process. You can get information about your installation options at \nhttp://docs.perfsonar.net/install_getting.html#gettingchooseinstall\n\n\nInstallation\n\n\n\n\nFor sites using perfSONAR Net Install - please follow \nhttp://docs.perfsonar.net/install_centos_netinstall.html#step-by-step-guide\n\n\nFor sites using perfSONAR Full Install - please follow \nhttp://docs.perfsonar.net/install_centos_fullinstall.html#step-by-step-guide\n\n\nFor general configuration of perfSONAR - please follow \nhttp://docs.perfsonar.net/install_config_first_time.html\n\n\n\n\nFor sites the are currently registered but not yet updated to 4.0 we would strongly recommend reinstalling using CentOS 7.x (follow either the Full Install or the Net Install above). There is no need for sites to preserve their data from the OSG/WLCG perspective, since we have already been collecting your data centrally. The primary reason for this recommendation is that the next point release of perfSONAR (version 4.1) will no longer support RHEL6/CentOS6/Scientific Linux 6 as a supported operating system.\n\n\nOSG and WLCG Configuration Notes\n\n\nAll perfSONAR instances for use in OSG and WLCG should be registered either in OIM or GOCDB. Please verify or register your instances at:\n\n\n\n\nFor OSG, follow the details in \nOIM\n\n\nFor Non-OSG, follow the details in \nGOCDB\n\n\n\n\nPlease ensure you have added or updated your administrative information: \nhttp://docs.perfsonar.net/manage_admin_info.html\n\n\n\n\nAdding communities is optional, but we recommend putting in WLCG as well as your VO: \nATLAS\n, \nCMS\n, etc. This just helps others from the community lookup your instances in the lookup service. As noted in the documentation you can select from already registered communities as appropriate.\n\n\n\n\nAssuming your registration is for a \nnew\n node or if you have changed the nodes FQDN, you will need to email \nwlcg-perfsonar-support 'at' cern.ch\n or open GGUS ticket for \nWLCG perfSONAR support unit\n so that your node can be added to one or more meshes. Please indicate if you have preferences for which meshes your node should be included in.\n\n\nYou will need to configure your instance(s) to use the OSG/WLCG mesh-configuration. OSG provides MCA (Mesh Configuration Adminstrator) GUI (see \nhttp://docs.perfsonar.net/mca.html\n for details) to centrally define the network tests that need to be run. Each perfSONAR toolkit installation for OSG/WLCG should add the \"auto\" mesh URL in their \n/etc/perfsonar/meshconfig-agent.conf\n file:\n\n\n\n\nSet this to \nhttp://meshconfig.grid.iu.edu/pub/auto/\nFQDN\n Replace \nFQDN\n with the fully qualified domain name of your host, e.g., \npsum01.aglt2.org\n.\n\n\n\n\nBelow is an example set of lines for meshconfig-agent.conf\n\n\nmesh\n \n    configuration_url http://meshconfig.grid.iu.edu/pub/auto/psum01.aglt2.org\n    validate_certificate 0 \n    required 1 \n  \n/mesh\n   \n  # Replace the following with suitable values for your installation \n  address psum01.aglt2.org \n  admin_email smckee@umich.edu \n  skip_redundant_tests 1\n\n\n\n\n\n\n\n\nNote\n\n\nUntil your host is added (on http://meshconfig.grid.iu.edu ) to one or more meshes by a mesh-config administrator, the automesh configuration above won't be returning any tests (See registration information above).\n\n\n\n\nWe \nstrongly recommend\n configuring perfSONAR in \ndual-stack mode\n (both IPv4 and IPv6)\n\n\n\n\nIn case your site has IPv6 support, the only necessary step is to get both A and AAAA records for your perfSONAR DNS names (as well as ensuring the reverse is in place).\n\n\nAll existing meshes will support both IPv4 and IPv6 testing. Sites with both IPv4 and IPv6 addresses testing to sites that also have both will run \ntwo\n tests. A side effect of this is that as more sites provide IPv6 addresses, the amount of testing will increase.\n\n\nAt some future point when most sites have IPv6, we may need to adjust the testing frequency to reduce the overall amount of testing\n\n\nFor more information on IPv6 see \nhttp://ipv6.web.cern.ch/\n\n\n\n\nSecurity Considerations\n\n\n\n\nWarning\n\n\nAs of the release of perfSONAR 4.0 ALL perfSONAR instances need to have port 443 access to all other perfSONAR instances. This change is because of the new requirements introduced by pScheduler. If sites are unable to reach your instance on port 443, tests may not run and results may not be available.\n\n\n\n\nThe perfSONAR toolkit is reviewed both internally and externally for security flaws. The toolkit's purpose is to allow us to measure and diagnose network problems and we therefore need to be cautious about blocking needed functionality by site or host firewalls.\n\n\nSome sites are concerned about having port 80 and/or 443 open. The working group would like to emphasize that these ports provide access to the perfSONAR web interface and are very useful to users and network administrators. \nOur recommendation is to keep them open\n, but for sites with strong concerns we have some rules documented below to customize iptables to block ports 80 and 443. It is \nrequired\n that port 80 \nand\n port 443 be accessible from the OSG and WLCG monitoring subnets listed below. In addition port 443 \nmust\n be accessible to all other perfSONAR instances that your node will test to. This is a new requirement as of the release of perfSONAR 4.0. \n\n\nBelow are example iptable rules to implement OSG/WLCG required access:\n\n\n# Port 443 must be open \niptables -I INPUT 4 -p tcp --dport 443 -j ACCEPT\n# Allow port 80 for specific monitoring subnets AT A MINIMUM (we recommend opening port 80 so others can view/access your perfSONAR Toolkit web GUI) \n# OSG monitoring subnet \niptables -I INPUT 4 -p tcp --dport 80 -s 129.79.53.0/24 -j ACCEPT \n# CERN subnet \niptables -I INPUT 4 -p tcp --dport 80 -s 137.138.0.0/17 -j ACCEPT \n# Infrastructure monitoring (hosted at AGLT2) \niptables -I INPUT 4 -p tcp --dport 80 -s 192.41.231.110/32 -j ACCEPT \n# Replace \nLOCAL\\_SUBNET\n appropriately to allow access from your local systems \niptables -I INPUT 4 -p tcp --dport 80 -s \nLOCAL\\_SUBNET\n -j ACCEPT \n# Reject ONLY if your site policy requires this \n#iptables -I INPUT 5 -p tcp --dport 80 -j REJECT\n\n\n\n\n\nTo save your changes run \n/sbin/service iptables save\n \n\n\n\n\nWarning\n\n\nIn case you have \ncentral/campus firewall\n, please check the required port openings in the perfSONAR documentation at \nhttp://docs.perfsonar.net/manage_security.html\n  \n\n\n\n\nFor any further questions, please consult \nTroubleshooting\n pages, perfSONAR documentation (\nhttp://docs.perfsonar.net\n) or contact \nWLCG perfSONAR Monitoring Support Unit\n.\n\n\n\n\n\n\n-- Main.ShawnMcKee - 17-Oct-2017", 
            "title": "Installation and Operations"
        }, 
        {
            "location": "/perfsonar/installation/#installation-and-upgrade-details", 
            "text": "This page documents installing or upgrading  perfSONAR  for OSG and WLCG sites. In case this is the first time you're trying to install and integrate your perfSONAR into WLCG or OSG, please consult  https://opensciencegrid.github.io/networking/perfsonar-in-osg/  for an overview of perfSONAR in OSG.  For any questions or help with WLCG perfSONAR setup, please email  wlcg-perfsonar-support 'at' cern.ch  or open GGUS ticket for  WLCG perfSONAR Monitoring Support Unit . We strongly recommend anyone maintaining/using perfSONAR to join  https://lists.internet2.edu/sympa/subscribe/perfsonar-user  and  https://lists.internet2.edu/sympa/subscribe/perfsonar-announce  Please follow the instructions for  upgrading  or  installing  perfSONAR as appropriate for your plans.", 
            "title": "Installation and Upgrade Details"
        }, 
        {
            "location": "/perfsonar/installation/#updating-perfsonar", 
            "text": "Before updating, please read the release notes at  http://docs.perfsonar.net/manage_update.html#special-upgrade-notes  To update please run the following:  # yum clean all\n# yum update\n# reboot  Note, it's  very important to reboot  otherwise your update is incomplete. Please also note, that starting from v3.4  yum  auto-updates are turned on by default.  We strongly recommend keeping them turned on . With  yum  auto-updates in place there is a possibility that updated packages can \"break\" your perfSONAR install but this is viewed an acceptable risk in order to have security updates quickly applied on perfSONAR instances.  If you haven't yet added another ( non-root ) user to administer the web interface, you should run  /usr/lib/perfsonar/scripts/nptoolkit-configure.py  and do so, as suggested in  motd . See  http://docs.perfsonar.net/manage_users.html  for detailed instructions.  Please backup your old  /etc/perfsonar/meshconfig-agent.conf  file and replace it with  /etc/perfsonar/meshconfig-agent.conf.rpmnew .  Then please verify/reconfigure your boxes following  OSG and WLCG Configuration . For any further questions, please consult  Troubleshooting  pages, perfSONAR documentation ( http://docs.perfsonar.net ) or contact directly WLCG or OSG perfSONAR support units.", 
            "title": "Updating perfSONAR"
        }, 
        {
            "location": "/perfsonar/installation/#installing-perfsonar", 
            "text": "perfSONAR 4.0 was released in April 2017. For sites running the previous version (3.x.y) the update should have automatically happened assuming that yum auto-updates were enabled and the  /etc/yum.repos.d/Internet2.repo  had 'enabled = 1'.  The following information details how to install perfSONAR 4.0 for WLCG and OSG. For general troubleshooting please go  here , in case you have any questions please email  wlcg-perfsonar-support 'at' cern.ch  or open GGUS ticket for  WLCG perfSONAR support unit . For OSG sites that don't participate in WLCG, please contact  OSG operations .  The perfSONAR team has created very good documentation on the process. You can get information about your installation options at  http://docs.perfsonar.net/install_getting.html#gettingchooseinstall", 
            "title": "Installing perfSONAR"
        }, 
        {
            "location": "/perfsonar/installation/#installation", 
            "text": "For sites using perfSONAR Net Install - please follow  http://docs.perfsonar.net/install_centos_netinstall.html#step-by-step-guide  For sites using perfSONAR Full Install - please follow  http://docs.perfsonar.net/install_centos_fullinstall.html#step-by-step-guide  For general configuration of perfSONAR - please follow  http://docs.perfsonar.net/install_config_first_time.html   For sites the are currently registered but not yet updated to 4.0 we would strongly recommend reinstalling using CentOS 7.x (follow either the Full Install or the Net Install above). There is no need for sites to preserve their data from the OSG/WLCG perspective, since we have already been collecting your data centrally. The primary reason for this recommendation is that the next point release of perfSONAR (version 4.1) will no longer support RHEL6/CentOS6/Scientific Linux 6 as a supported operating system.", 
            "title": "Installation"
        }, 
        {
            "location": "/perfsonar/installation/#osg-and-wlcg-configuration-notes", 
            "text": "All perfSONAR instances for use in OSG and WLCG should be registered either in OIM or GOCDB. Please verify or register your instances at:   For OSG, follow the details in  OIM  For Non-OSG, follow the details in  GOCDB   Please ensure you have added or updated your administrative information:  http://docs.perfsonar.net/manage_admin_info.html   Adding communities is optional, but we recommend putting in WLCG as well as your VO:  ATLAS ,  CMS , etc. This just helps others from the community lookup your instances in the lookup service. As noted in the documentation you can select from already registered communities as appropriate.   Assuming your registration is for a  new  node or if you have changed the nodes FQDN, you will need to email  wlcg-perfsonar-support 'at' cern.ch  or open GGUS ticket for  WLCG perfSONAR support unit  so that your node can be added to one or more meshes. Please indicate if you have preferences for which meshes your node should be included in.  You will need to configure your instance(s) to use the OSG/WLCG mesh-configuration. OSG provides MCA (Mesh Configuration Adminstrator) GUI (see  http://docs.perfsonar.net/mca.html  for details) to centrally define the network tests that need to be run. Each perfSONAR toolkit installation for OSG/WLCG should add the \"auto\" mesh URL in their  /etc/perfsonar/meshconfig-agent.conf  file:   Set this to  http://meshconfig.grid.iu.edu/pub/auto/ FQDN  Replace  FQDN  with the fully qualified domain name of your host, e.g.,  psum01.aglt2.org .   Below is an example set of lines for meshconfig-agent.conf  mesh  \n    configuration_url http://meshconfig.grid.iu.edu/pub/auto/psum01.aglt2.org\n    validate_certificate 0 \n    required 1 \n   /mesh    \n  # Replace the following with suitable values for your installation \n  address psum01.aglt2.org \n  admin_email smckee@umich.edu \n  skip_redundant_tests 1     Note  Until your host is added (on http://meshconfig.grid.iu.edu ) to one or more meshes by a mesh-config administrator, the automesh configuration above won't be returning any tests (See registration information above).   We  strongly recommend  configuring perfSONAR in  dual-stack mode  (both IPv4 and IPv6)   In case your site has IPv6 support, the only necessary step is to get both A and AAAA records for your perfSONAR DNS names (as well as ensuring the reverse is in place).  All existing meshes will support both IPv4 and IPv6 testing. Sites with both IPv4 and IPv6 addresses testing to sites that also have both will run  two  tests. A side effect of this is that as more sites provide IPv6 addresses, the amount of testing will increase.  At some future point when most sites have IPv6, we may need to adjust the testing frequency to reduce the overall amount of testing  For more information on IPv6 see  http://ipv6.web.cern.ch/", 
            "title": "OSG and WLCG Configuration Notes"
        }, 
        {
            "location": "/perfsonar/installation/#security-considerations", 
            "text": "Warning  As of the release of perfSONAR 4.0 ALL perfSONAR instances need to have port 443 access to all other perfSONAR instances. This change is because of the new requirements introduced by pScheduler. If sites are unable to reach your instance on port 443, tests may not run and results may not be available.   The perfSONAR toolkit is reviewed both internally and externally for security flaws. The toolkit's purpose is to allow us to measure and diagnose network problems and we therefore need to be cautious about blocking needed functionality by site or host firewalls.  Some sites are concerned about having port 80 and/or 443 open. The working group would like to emphasize that these ports provide access to the perfSONAR web interface and are very useful to users and network administrators.  Our recommendation is to keep them open , but for sites with strong concerns we have some rules documented below to customize iptables to block ports 80 and 443. It is  required  that port 80  and  port 443 be accessible from the OSG and WLCG monitoring subnets listed below. In addition port 443  must  be accessible to all other perfSONAR instances that your node will test to. This is a new requirement as of the release of perfSONAR 4.0.   Below are example iptable rules to implement OSG/WLCG required access:  # Port 443 must be open \niptables -I INPUT 4 -p tcp --dport 443 -j ACCEPT\n# Allow port 80 for specific monitoring subnets AT A MINIMUM (we recommend opening port 80 so others can view/access your perfSONAR Toolkit web GUI) \n# OSG monitoring subnet \niptables -I INPUT 4 -p tcp --dport 80 -s 129.79.53.0/24 -j ACCEPT \n# CERN subnet \niptables -I INPUT 4 -p tcp --dport 80 -s 137.138.0.0/17 -j ACCEPT \n# Infrastructure monitoring (hosted at AGLT2) \niptables -I INPUT 4 -p tcp --dport 80 -s 192.41.231.110/32 -j ACCEPT \n# Replace  LOCAL\\_SUBNET  appropriately to allow access from your local systems \niptables -I INPUT 4 -p tcp --dport 80 -s  LOCAL\\_SUBNET  -j ACCEPT \n# Reject ONLY if your site policy requires this \n#iptables -I INPUT 5 -p tcp --dport 80 -j REJECT  To save your changes run  /sbin/service iptables save     Warning  In case you have  central/campus firewall , please check the required port openings in the perfSONAR documentation at  http://docs.perfsonar.net/manage_security.html      For any further questions, please consult  Troubleshooting  pages, perfSONAR documentation ( http://docs.perfsonar.net ) or contact  WLCG perfSONAR Monitoring Support Unit .    -- Main.ShawnMcKee - 17-Oct-2017", 
            "title": "Security Considerations"
        }, 
        {
            "location": "/perfsonar/faq/", 
            "text": "perfSONAR  FAQ and  Troubleshooting  for  OSG  and  WLCG  Instances\n\n\n Here we will provide details on troubleshooting perfSONAR installations for OSG and WLCG as well as some additional configuration options and a FAQ.\n\n\nThere are a good set of \ncli\n tools for perfSONAR available. Details on these tools are at \nhttps://twiki.grid.iu.edu/bin/view/Documentation/Release3/NetworkPerformanceToolkit\n The \nowping\n and \nbwctl\n tools can be very useful to test from your location to any perfSONAR instance running either the OWAMP or BWCTL services respectively.\n\n\nWe are maintaining a \nNetwork Troubleshooting\n Wiki page to guide users in identifying and following up on network problems.\n\n\nTo further secure access to your perfSONAR web interface, we are providing a \nsecuring the perfSONAR web\n page (not yet tested).\n\n\nFAQ\n\n\nMy perfSONAR disks are filling up with log messages.\n\n\nThe default logging in perfSONAR 3.4 produces a lot of output and some sites are filling their disks with logging data. This has been reported to the perfSONAR developers and there will be fixes coming in future versions. In the meantime you can reduce the amount of logging by doing the following. As \nroot\n you can locate all the relevant logger.conf files:\n\n\n[root@psum01 ~]# locate logger.conf\n/opt/SimpleLS/bootstrap/etc/SimpleLSBootStrapClientDaemon-logger.conf\n/opt/perfsonar_ps/ls_cache_daemon/etc/ls_cache_daemon-logger.conf\n/opt/perfsonar_ps/ls_registration_daemon/etc/ls_registration_daemon-logger.conf\n/opt/perfsonar_ps/regular_testing/etc/regular_testing-logger.conf\n/opt/perfsonar_ps/toolkit/etc/config_daemon-logger.conf\n/opt/perfsonar_ps/toolkit/etc/service_watcher-logger.conf\n/opt/perfsonar_ps/toolkit/web/root/admin/administrative_info/etc/logger.conf\n/opt/perfsonar_ps/toolkit/web/root/admin/enabled_services/etc/logger.conf\n/opt/perfsonar_ps/toolkit/web/root/admin/ntp/etc/logger.conf\n/opt/perfsonar_ps/toolkit/web/root/admin/regular_testing/etc/logger.conf\n/opt/perfsonar_ps/toolkit/web/root/gui/services/etc/logger.conf\n\n\n\n\n\nYou can move the logging levels for DEBUG and INFO up to WARN level via:\n\n\nsed -i \ns/DEBUG,/INFO,/g\n `locate logger.conf`\nsed -i \ns/INFO,/WARN,/g\n `locate logger.conf`\n\n\n\n\n\nComment out the 'verbose' line from \n/etc/owampd/owampd.conf\n\n\nFinally reboot:\n\n\nreboot\n\n\n\n\n\nInfrastructure Monitoring (check_mk metrics)\n\n\n\n\nperfSONAR 3.4+ Toolkit Version\n metric is failing\n\n\nThis metrics checks if your sonar is at particular version (as of Dec. 12, it checks if it's at 3.4.1). In case you're at version 3.4 (and not 3.4.1), please check if you have automatic yum updates enabled, this is strongly recommended due to security issues we have seen in the past. In case you're still running version an older version (3.3), please update and reconfigure as soon as possible following \nInstallation Guide\n 2. \nperfSONAR Administrator Details\n metric is failing 3. \nperfSONAR BWCTL Bandwidth Test Controller\n metric is failing\n\n\nThis means that we're unable to connect to your bandwidth controller port, please ensure you have correct firewall settings (especially white listed subnets allowed) as described in the \nInstallation Guide\n . This can also indicate failures of bwctl daemon, please check \nhttp://www.perfsonar.net/about/faq\n (e.g. \nhttp://www.perfsonar.net/about/faq/#Q22\n), you can ask for help by opening a GGUS ticket to WLCG perfSONAR support 4. \nperfSONAR esmond Measurement Archive\n metric is failing\n\n\nThis means that your measurement archive is not accessible, there can be many possible causes (disk full, httpd not running or inaccessible, etc.), you can ask for help by opening a GGUS ticket to WLCG perfSONAR support. 5. \nperfSONAR Homepage\n is failing\n\n\nThis means the toolkit's homepage is inaccessible, please check for usual causes (disk full, httpd not running or blocked), we need to be able to access your homepage via HTTP or HTTPS 7. \nperfSONAR Latitude/Longitude Configured\n is failing 8. \nperfSONAR Mesh Configuration\n is failing\n\n\nThis indicates that you're missing the recommended mesh configuration. Please configure your mesh URL(s) in \n/opt/perfsonar_ps/mesh_config/etc/agent_configuration.conf\n. We have a new \nauto-mesh\n capability now and all sites should set:\n\n\nSet this to \nhttp://meshconfig.grid.iu.edu/pub/auto/\nFQDN\n Replace \nFQDN\n with the fully qualified domain name of your host, e.g., \npsum01.aglt2.org\n. Values for each instance are \nin this list\n.\n\n\n\n\n\n\n\n\n\n\n configuration_url \nhttp://meshconfig.grid.iu.edu/pub/auto/psum01.aglt2.org\n validate_certificate 0 required 1 \n \n\n\n\n\nPlease REMOVE any old mesh configuration, this metric will also fail in case you have both the new mesh config and the old mesh URLs 9. \nperfSONAR NTP Service\n is failing\n\n\nThis indicates that NTP service is not running correctly on your sonar, please note that NTP is critical service. Please refer to \nhttp://www.perfsonar.net/about/faq/\n 10. \nperfSONAR Regular Testing Service\n is failing\n\n\nThis indicates that regular testing service is not working, please try to enable/disable it following \nhttp://docs.perfsonar.net/manage_regular_tests.html#disabling-enabling-regular-tests\n\n\nIn case the issue persists, please open ticket to WLCG perfSONAR SU in GGUS 11. \nperfSONAR Toolkit Version\n is failing\n\n\nThis metrics checks if your sonar is at particular version (as of Dec. 12, it checks if it's at 3.4.1). In case you're at version 3.4 (and not 3.4.1), please check if you have automatic yum updates enabled, this is strongly recommended due to security issues we have seen in the past. In case you're still running version an older version (3.3), please update and reconfigure as soon as possible following \nInstallation Guide\n 12. There are \nmany tests failing\n for given sonar, where should I start\n\n\nPlease update and reconfigure your sonar following \nInstallation Guide\n. Please ensure firewall doesn't block access from the whitelisted subnets that are required for the infrastructure monitoring to work. 13. \nWhere can I get support on managing WLCG perfSONAR\n\n\nYou can open ticket in GGUS to WLCG perfSONAR support unit or contact directly wlcg-perfsonar-support (at cern.ch) 14. \nperfSONAR esmond Freshness Latency/Bandwidth Direct\n is failing or gives warning\n\n\nThis metric checks freshness of the local measurement archive, in particular it checks if it contains fresh results for all the configured tests. This metric is needed to determine if we're able to consistently get results from perfSONAR boxes in WLCG. Currently it's a non-critical test, you can ignore it. 15. \nperfSONAR esmond Freshness Latency/Bandwidth Reverse\n is failing or gives warning\n\n\nThis metric checks freshness of the local measurement archive, in particular it checks if it contains fresh results for all the configured reverse tests (reverse test is any test that is done from remote to local sonar). This metric is needed to determine if we're able to consistently get results from perfSONAR boxes in WLCG. Currently it's a non-critical test, you can ignore it. 16. \nperfSONAR NDT HTTP Network Diagnostic Tester\n and/or \nperfSONAR NDT P Network Diagnostic Tester\n and/or \nperfSONAR NPAD Network Path and Application Diagnosis\n are failing\n\n\nBoth metrics check if you have disabled NDT and NPAD as we recommend in the \nInstallation/Update Guide\n. Please follow the guide to disable this service. 17. \nperfSONAR Toolkit Memory\n is failing\n\n\nStarting with perfSONAR version 3.4, the minimal required memory is at least 4GBs. In case hardware update is not possible for your sonars, we can still use them to run on demand tests for debugging purposes, but we won't be able to run any baselining activities.\n\n\n\n\n\n\n\n\n-- Main.ShawnMcKee - 16 Oct 2014", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/perfsonar/faq/#perfsonar-faq-and-troubleshooting-for-osg-and-wlcg-instances", 
            "text": "Here we will provide details on troubleshooting perfSONAR installations for OSG and WLCG as well as some additional configuration options and a FAQ.  There are a good set of  cli  tools for perfSONAR available. Details on these tools are at  https://twiki.grid.iu.edu/bin/view/Documentation/Release3/NetworkPerformanceToolkit  The  owping  and  bwctl  tools can be very useful to test from your location to any perfSONAR instance running either the OWAMP or BWCTL services respectively.  We are maintaining a  Network Troubleshooting  Wiki page to guide users in identifying and following up on network problems.  To further secure access to your perfSONAR web interface, we are providing a  securing the perfSONAR web  page (not yet tested).", 
            "title": "perfSONAR  FAQ and  Troubleshooting  for  OSG  and  WLCG  Instances"
        }, 
        {
            "location": "/perfsonar/faq/#faq", 
            "text": "", 
            "title": "FAQ"
        }, 
        {
            "location": "/perfsonar/faq/#my-perfsonar-disks-are-filling-up-with-log-messages", 
            "text": "The default logging in perfSONAR 3.4 produces a lot of output and some sites are filling their disks with logging data. This has been reported to the perfSONAR developers and there will be fixes coming in future versions. In the meantime you can reduce the amount of logging by doing the following. As  root  you can locate all the relevant logger.conf files:  [root@psum01 ~]# locate logger.conf\n/opt/SimpleLS/bootstrap/etc/SimpleLSBootStrapClientDaemon-logger.conf\n/opt/perfsonar_ps/ls_cache_daemon/etc/ls_cache_daemon-logger.conf\n/opt/perfsonar_ps/ls_registration_daemon/etc/ls_registration_daemon-logger.conf\n/opt/perfsonar_ps/regular_testing/etc/regular_testing-logger.conf\n/opt/perfsonar_ps/toolkit/etc/config_daemon-logger.conf\n/opt/perfsonar_ps/toolkit/etc/service_watcher-logger.conf\n/opt/perfsonar_ps/toolkit/web/root/admin/administrative_info/etc/logger.conf\n/opt/perfsonar_ps/toolkit/web/root/admin/enabled_services/etc/logger.conf\n/opt/perfsonar_ps/toolkit/web/root/admin/ntp/etc/logger.conf\n/opt/perfsonar_ps/toolkit/web/root/admin/regular_testing/etc/logger.conf\n/opt/perfsonar_ps/toolkit/web/root/gui/services/etc/logger.conf  You can move the logging levels for DEBUG and INFO up to WARN level via:  sed -i  s/DEBUG,/INFO,/g  `locate logger.conf`\nsed -i  s/INFO,/WARN,/g  `locate logger.conf`  Comment out the 'verbose' line from  /etc/owampd/owampd.conf  Finally reboot:  reboot", 
            "title": "My perfSONAR disks are filling up with log messages."
        }, 
        {
            "location": "/perfsonar/faq/#infrastructure-monitoring-check95mk-metrics", 
            "text": "perfSONAR 3.4+ Toolkit Version  metric is failing  This metrics checks if your sonar is at particular version (as of Dec. 12, it checks if it's at 3.4.1). In case you're at version 3.4 (and not 3.4.1), please check if you have automatic yum updates enabled, this is strongly recommended due to security issues we have seen in the past. In case you're still running version an older version (3.3), please update and reconfigure as soon as possible following  Installation Guide  2.  perfSONAR Administrator Details  metric is failing 3.  perfSONAR BWCTL Bandwidth Test Controller  metric is failing  This means that we're unable to connect to your bandwidth controller port, please ensure you have correct firewall settings (especially white listed subnets allowed) as described in the  Installation Guide  . This can also indicate failures of bwctl daemon, please check  http://www.perfsonar.net/about/faq  (e.g.  http://www.perfsonar.net/about/faq/#Q22 ), you can ask for help by opening a GGUS ticket to WLCG perfSONAR support 4.  perfSONAR esmond Measurement Archive  metric is failing  This means that your measurement archive is not accessible, there can be many possible causes (disk full, httpd not running or inaccessible, etc.), you can ask for help by opening a GGUS ticket to WLCG perfSONAR support. 5.  perfSONAR Homepage  is failing  This means the toolkit's homepage is inaccessible, please check for usual causes (disk full, httpd not running or blocked), we need to be able to access your homepage via HTTP or HTTPS 7.  perfSONAR Latitude/Longitude Configured  is failing 8.  perfSONAR Mesh Configuration  is failing  This indicates that you're missing the recommended mesh configuration. Please configure your mesh URL(s) in  /opt/perfsonar_ps/mesh_config/etc/agent_configuration.conf . We have a new  auto-mesh  capability now and all sites should set:  Set this to  http://meshconfig.grid.iu.edu/pub/auto/ FQDN  Replace  FQDN  with the fully qualified domain name of your host, e.g.,  psum01.aglt2.org . Values for each instance are  in this list .       configuration_url  http://meshconfig.grid.iu.edu/pub/auto/psum01.aglt2.org  validate_certificate 0 required 1      Please REMOVE any old mesh configuration, this metric will also fail in case you have both the new mesh config and the old mesh URLs 9.  perfSONAR NTP Service  is failing  This indicates that NTP service is not running correctly on your sonar, please note that NTP is critical service. Please refer to  http://www.perfsonar.net/about/faq/  10.  perfSONAR Regular Testing Service  is failing  This indicates that regular testing service is not working, please try to enable/disable it following  http://docs.perfsonar.net/manage_regular_tests.html#disabling-enabling-regular-tests  In case the issue persists, please open ticket to WLCG perfSONAR SU in GGUS 11.  perfSONAR Toolkit Version  is failing  This metrics checks if your sonar is at particular version (as of Dec. 12, it checks if it's at 3.4.1). In case you're at version 3.4 (and not 3.4.1), please check if you have automatic yum updates enabled, this is strongly recommended due to security issues we have seen in the past. In case you're still running version an older version (3.3), please update and reconfigure as soon as possible following  Installation Guide  12. There are  many tests failing  for given sonar, where should I start  Please update and reconfigure your sonar following  Installation Guide . Please ensure firewall doesn't block access from the whitelisted subnets that are required for the infrastructure monitoring to work. 13.  Where can I get support on managing WLCG perfSONAR  You can open ticket in GGUS to WLCG perfSONAR support unit or contact directly wlcg-perfsonar-support (at cern.ch) 14.  perfSONAR esmond Freshness Latency/Bandwidth Direct  is failing or gives warning  This metric checks freshness of the local measurement archive, in particular it checks if it contains fresh results for all the configured tests. This metric is needed to determine if we're able to consistently get results from perfSONAR boxes in WLCG. Currently it's a non-critical test, you can ignore it. 15.  perfSONAR esmond Freshness Latency/Bandwidth Reverse  is failing or gives warning  This metric checks freshness of the local measurement archive, in particular it checks if it contains fresh results for all the configured reverse tests (reverse test is any test that is done from remote to local sonar). This metric is needed to determine if we're able to consistently get results from perfSONAR boxes in WLCG. Currently it's a non-critical test, you can ignore it. 16.  perfSONAR NDT HTTP Network Diagnostic Tester  and/or  perfSONAR NDT P Network Diagnostic Tester  and/or  perfSONAR NPAD Network Path and Application Diagnosis  are failing  Both metrics check if you have disabled NDT and NPAD as we recommend in the  Installation/Update Guide . Please follow the guide to disable this service. 17.  perfSONAR Toolkit Memory  is failing  Starting with perfSONAR version 3.4, the minimal required memory is at least 4GBs. In case hardware update is not possible for your sonars, we can still use them to run on demand tests for debugging purposes, but we won't be able to run any baselining activities.     -- Main.ShawnMcKee - 16 Oct 2014", 
            "title": "Infrastructure Monitoring (check_mk metrics)"
        }, 
        {
            "location": "/network-troubleshooting/", 
            "text": "Network  and Network Troubleshooting Documentation\n\n\n This page will eventually include pointers to various network troubleshooting resources. In general, when users suspect a network problem the procedure is:\n\n\n\n\nDocument the problem\n: Basically you need to describe the problem you encountered. Provide any relevant details like the exact command you used, any errors or warning you got and any problems you observed. This can help you better understand the problem and will allow you to easily \"hand-off\" troubleshooting to an expert\n\n\nGather relevant data:\n Run tests (see Guide below) and capture the results.\n\n\nContact your local network support\n (this is sometimes where users don't know where to go). Google or your campus web-pages should be able to help you find the right contact.\n\n\nEscalate\n, if the problem persists (See below)\n\n\n\n\nESnet maintains a very useful page on network troubleshooting at \nhttps://fasterdata.es.net/performance-testing/troubleshooting/\n\n\nWe have a (older) draft version of the OSG \nnetwork debugging document\n that describes things in much more detail. If you have comments, questions or suggestions, please contact Shawn McKee.\n\n\nIf you want to learn more about perfSONAR and its various components, the Network Startup Resource Center maintains a list of training videos at \nhttps://learn.nsrc.org/perfsonar\n\n\nInformation on Contacting Network Support\n\n\nThere are numerous regional and campus network operations centers that have their own ticketing systems for reporting problems. I encourage you to identify how to contact your local campus network support personnel. Suggestion: try using Google with \"YourUniversity network trouble ticket\" as the search terms (of course substitute your university name for \nYourUniversity\n).\n\n\nBelow are the links you can use to report problems to OSG or the major Research and Education networks. If you can't determine who your local contact should be or they are unable to help you resolve the issue, you should open a ticket with one of these entities:\n\n\nINTERNET2\n: If you are located at a University in the United States you are likely connected to Internet2. You can find the details on opening a ticket with Internet2 at:\n\n\n\n\nhttp://noc.net.internet2.edu/i2network/support.html\n\n\n\n\nThis is typically a good choice for support beyond your campus.\n\n\nESNET\n: If your problem involves a national laboratory in the United States, a university connected to ESnet or a trans-Atlantic network problem, then you may want to contact ESnet:\n\n\n\n\nhttp://www.es.net/introducing-esnet5/tools/\n\n\n\n\nThis page contains information on opening ticket and pointers to relevant tools and documentation. ESnet serves the Department of Energy national labs.\n\n\nOSG\n: If you have questions or unusual problems you think are OSG related, feel free to contact the OSG GOC. You can also open network related tickets and OSG can help to 'triage' your request and get it to the right people: \n\n\n\n\nhttps://opensciencegrid.github.io/production/\n\n\n\n\n\n\n\n\n-- Main.ShawnMcKee - 09 Sep 2017", 
            "title": "Network Troubleshooting"
        }, 
        {
            "location": "/network-troubleshooting/#network-and-network-troubleshooting-documentation", 
            "text": "This page will eventually include pointers to various network troubleshooting resources. In general, when users suspect a network problem the procedure is:   Document the problem : Basically you need to describe the problem you encountered. Provide any relevant details like the exact command you used, any errors or warning you got and any problems you observed. This can help you better understand the problem and will allow you to easily \"hand-off\" troubleshooting to an expert  Gather relevant data:  Run tests (see Guide below) and capture the results.  Contact your local network support  (this is sometimes where users don't know where to go). Google or your campus web-pages should be able to help you find the right contact.  Escalate , if the problem persists (See below)   ESnet maintains a very useful page on network troubleshooting at  https://fasterdata.es.net/performance-testing/troubleshooting/  We have a (older) draft version of the OSG  network debugging document  that describes things in much more detail. If you have comments, questions or suggestions, please contact Shawn McKee.  If you want to learn more about perfSONAR and its various components, the Network Startup Resource Center maintains a list of training videos at  https://learn.nsrc.org/perfsonar", 
            "title": "Network  and Network Troubleshooting Documentation"
        }, 
        {
            "location": "/network-troubleshooting/#information-on-contacting-network-support", 
            "text": "There are numerous regional and campus network operations centers that have their own ticketing systems for reporting problems. I encourage you to identify how to contact your local campus network support personnel. Suggestion: try using Google with \"YourUniversity network trouble ticket\" as the search terms (of course substitute your university name for  YourUniversity ).  Below are the links you can use to report problems to OSG or the major Research and Education networks. If you can't determine who your local contact should be or they are unable to help you resolve the issue, you should open a ticket with one of these entities:  INTERNET2 : If you are located at a University in the United States you are likely connected to Internet2. You can find the details on opening a ticket with Internet2 at:   http://noc.net.internet2.edu/i2network/support.html   This is typically a good choice for support beyond your campus.  ESNET : If your problem involves a national laboratory in the United States, a university connected to ESnet or a trans-Atlantic network problem, then you may want to contact ESnet:   http://www.es.net/introducing-esnet5/tools/   This page contains information on opening ticket and pointers to relevant tools and documentation. ESnet serves the Department of Energy national labs.  OSG : If you have questions or unusual problems you think are OSG related, feel free to contact the OSG GOC. You can also open network related tickets and OSG can help to 'triage' your request and get it to the right people:    https://opensciencegrid.github.io/production/     -- Main.ShawnMcKee - 09 Sep 2017", 
            "title": "Information on Contacting Network Support"
        }, 
        {
            "location": "/network-troubleshooting/osg-debugging-document/", 
            "text": "OSG Debugging Documentation\n\n\nEdited By: J. Zurawski \u2013 Internet2, S. McKee \u2013 University of Michigan\n\n\nFebruary 4\n\n\n# th\n \n2013\n\n\n\n\n1.Abstract\n\n\n\n\nScientific progress relies on intricate systems of computers and the networks that connect them.  Often it is the case that scientific data is gathered via a well defined process, information is digitized, stored, transmitted, and processed by members of large and distributed collaborations.  The Open Science Grid advances science through the concept of distributed computing \u2013 the process for sharing resources through a unified software framework focused on the common tasks of data movement, processing, and analysis.\n\n\nNetworks are an integral part of the distributed computing process.  Similar to the computational and storage resources, it is crucial that all networking components, on the complete end-to-end path, are functional and free of physical and logical flaws.  A rich set of measurement and monitoring tools exists to provide network operations staff and end users a window into the functionality of networks, despite the fact that these actors do not have direct control over the complete path their data may travel.\n\n\nThis document discusses common measurement and monitoring tools available to the OSG community, and strategies to deploy, use, and interpret the results they produce.  The end goal is to give end users more insight into network behavior, and assist local and remote networking staffs as they correct damaging performance problems that will impact the scientific process.\n\n\n\n\n2.Introduction\n\n\n\n\nThe process of science is often complicated when viewed as a complete system.  At the core of any project, there is a mechanism to observe or simulate some system, and produce meaningful results that will be interpreted and scrutinized between experimentation runs.   The machinery that surrounds this process can be as benign as simple cameras, or as complex as the Large Hadron Collider and its associated experiments.  Other common components include ways to digitize, store, process, and share the end result of experimentation \u2013 often done using computational systems.\n\n\nComputation falls into 3 broad classifications, all of which are required to implement the paradigm of scientific computing:\n\n\n\n\nStorage\n \u2013 Readable and writable physical medium used for temporary or long term residency of gathered data.\n\n\nProcessing\n \u2013 Specially designed hardware and software that iterates over collected data sets looking for pre-defined triggers and results.\n\n\nNetworking\n \u2013 Interconnecting hardware and software used to facilitate communication between storage and processing components both on a local, and fully distributed basis.\n\n\n\n\nWhen fully realized, even a small facility can contribute a great amount of resources to the overall goal of scientific advancement.  In practice it may be the case that a lab consisting of a single researcher can pull data sets from a centralized location, perform carefully selected segments of an entire set of analysis that is required, and return any relevant results as they are discovered.  When used in an inductive fashion, one can imagine the overall throughout that a VO such as the LHC project is able to attain through 100s of distributed facilities and 1000s of collaborating researchers.\n\n\nComplexity is present as we travel down the individual technology items in the above scenario.  Often it is the case that ideal performance is hard to attain due to the intertwined nature of the mechanisms involved.  For example, data must be written and read from physical medium.  Often this step is slower due to the mechanical nature of the process, and struggles to keep up with faster technologies such as processing or transmission on network infrastructure.  Equally, it may be the case that a flaw in the network infrastructure, such as a failing component, can introduce data loss that must be compensated for through retransmission.  Retransmission implies additional work for storage and processing components that must waste resources to overcome a fixable, but often unnoticed, problem.\n\n\nNetwork performance monitoring is a relatively unseen, but still extremely necessary, practice.  This statement is true due to the nature of network use through application software and communication protocols.  Application developers wish to unburden the user with details about \nhow\n data may be moved between facilities.  Care is taken to design applications in such a way that the user is simply presented with options related to a source and destination only, and little or no insight into the path taken or the current conditions that may be present.  The aforementioned situation where a failing component institutes data loss results in only one symptom to the end user: lower than expected throughput.  Many users may not notice, or have become complacent, with low performance situations.  Some may write this off as \nthe network is slow\n, or perhaps will not notice at all due to experiences with home connections that are often 2 orders of magnitude slower than what is possible in a typical academic environment.\n\n\nSoftware exists to monitor network performance in many different ways.  For example it is possible take a measure of network throughput, and simulate the behavior of a file transfer application.  It is also possible to observe network stability (e.g. jitter) over time to simulate video or audio transmission.  These basic observations are powerful when used both in a local environment, as well as on an end-to-end basis.  In either case, software must be deployed and available to the community on points of interest: specifically on the local and shared network infrastructure distributed around the world.  perfSONAR is a software framework that simplifies network debugging activities by making it easier to deploy measurement tools, and facilities the sharing of results.  It is currently deployed on many communal networking resources in the R\nE community, including backbones, regional networks, campuses, and individual laboratories.\n\n\nOnce perfSONAR is deployed, it becomes possible to troubleshoot situations that result in low throughout for the end user in a straightforward manner.  It is important to note that when something like this occurs, it is not the sole responsibility of the end user to debug and solve a networking problem; rather it is their duty to report the problem and provide as much information as possible to local or remote network staffs so they may learn about the issue, and interpret the results so as to lead to quick and efficient problem resolution.  Locating this staff may be challenging, but many organizations maintain a dedicated Network Operations Center (NOC) whose staff are ready to accept trouble reports and act in an appropriate manner.    Section 8 details locations you may turn to for additional support.\n\n\nThis document will expand upon these topics in the remaining sections, and conclude with information where additional resources beyond a simple introduction to these topics can be found.\n\n\n\n\n3.The Scientific Networking Process\n\n\n\n\nThere is a rich ecosystem of components available for monitoring and managing the scientific networking process.  This myriad of hardware and software must work together to complete the overall goal of interpreting gathered or simulated observations.  Each component we will discuss has the ability to be installed, operated, and maintained in different ways.  Individual brands or versions are not important, but the overall idea of each will be explored.\n\n\n\n\n3.1.Hosts\n\n\n\n\nServer or \nhost\n hardware and software can be used in many different ways.  Often it is the case that these components serve as computational resources for processing data, or provide access to underlying data stored on physical media.  It may also be the case that software designed to \nglue\n components of a framework together (e.g. processing mail, authenticating users, providing mappings between names and addresses) is installed on a dedicated or shared host resource.\n\n\nHosts must contain an operating system: software designed to control and maintain the underlying hardware such as storage media, network interface cards, processors, and other peripheral devices.  Operating system hardware can vary in functionality; completely interactive systems such as those found on laptops can be more pleasing for a human user vs. that of a no frills batch processing system designed to only iterate over scientific data.  The choice of operating system will vary from use case to use case.\n\n\nThe footing provided by the hardware and operating system serves as the basis for the remainder of the components in this discussion.\n\n\n\n\n3.2.Protocols\n\n\n\n\nProtocols are software algorithms implemented on hosts and networking components, and are used to facilitate communication strategies.  Protocols are constructed in a \nlayered\n fashion, and are designed to handle certain aspects of the overall communication plan.  For instance a protocol may be used to communicate between two network devices, and may involve the use of different patterns of electrical or optical signal.  On top of this basic system of signals we may construct a different protocol that is focused on communication between end hosts, and is able to break up the notion of a user\ns files into small chunks so they can be sent reliably end-to-end.\n\n\nProtocols evolve with the underlying technology, and often can be tuned to specific use cases.  One such protocol, Transmission Control Protocol (TCP), is widely used in applications that many users are familiar with such as web browsing, mail transfer, and file exchange.  Early incarnations of TCP were designed for the networks of the 1980s; slower, less reliable, constructions than what is present in the networks of the 2010s.  As such TCP must be instructed, via configuration on a host or network devices operating environment,  that it should behave in a different and more efficient manner.\n\n\nWith the protocol in place, we can now begin to discuss applications that are able to use the network to communicate in an automated fashion.\n\n\n\n\n3.3.Applications\n\n\n\n\nApplications are specific use cases, programmed as software, and made available to end users via a host\ns operating environment.  There are numerous applications we use on a daily basis \u2013 web browsers to fetch remote content, word processing applications to type papers, mail and instant messaging clients to exchange information in near real time.  Scientific applications normally focus on performing a single task (e.g. end-to-end data movement, visualization, data transformation, data analysis) on either a local or distributed basis.\n\n\nIn the case of distribution, it becomes necessary to interact with the underlying network through the use of a protocol.  File transfer is a specialized application that takes is interested in either sending a local file to a remote location, or retrieving remote data to bring locally.  In either case the application must broker with a protocol, such as TCP, that is available on both ends of the transfer.  Through a series of API calls information is segmented into transmission chunks and sent reliably though the communication medium.  Most of this is handled transparently from the user\ns perspective, and as such they are not given much in the way of feedback beyond a pass or failure, and some notion of how long it took.\n\n\nUnderstanding more about the network can be enlightening exercise for users who are unaware of the complexity and span of components that are required for operation.  This will be discussed in the remaining sections.\n\n\n\n\n3.4.Lab Local Area Network\n\n\n\n\nThe first step in the networking tree is often the interconnections between components local to the user.  This may consist of the storage and processing nodes in a single rack or data center used for scientific processing, connected via technology consistent with the tasks they are performing.  Cluster nodes may use a high speed interconnect such as InfiniBand; servers may also just use typical Ethernet at 1Gbps or 10Gbps.\n\n\nIn either case, there will be dedicated network equipment with the task of aggregating and controlling traffic flow to the local devices, and serving as an uplink to the next network in the chain (the campus).   Monitoring and management of this local environment is a good idea, either through passive means such as using the SNMP system, or active tests that check the health of transfers on a local basis.\n\n\n\n\n3.5.Campus Local Area Network\n\n\n\n\nThe first hop beyond a laboratory environment is a network maintained by campus/local support staff.  It is often the case that this group is maintaining the infrastructure for the use of \nall\n end users, and as such will design and maintain things to preserve uniform functionality and performance.\n\n\nCampus networks are an even larger ecosystem of devices given the area they may cover.  It may be the case that the network in the previous section is behind several devices before it has a clean path to the outside world.  It may also be the case that traffic aggregation is extremely high, and congestion becomes a factor during certain parts of the day or times of the year.  These nuances make local performance monitoring crucial for operational soundness.\n\n\nThis group is also the first contact that should be exercised in the event of a network performance problem.  While they may not be able to answer for the status of the entire path, they can escalate the problem to the regional or backbone support staff as needed.\n\n\n\n\n3.6.Regional Network\n\n\n\n\nA regional network provider aggregates the networks of numerous campuses in a state, country, or pre-defined region.  Examples include provider for states of the US (e.g. KanREN), countries (SWITCH, the network of Switzerland), or collaboration between parties without a political boundary (the SOX regional network in the United States).  Regional networks may cover a large geographical area, but often have less equipment than a campus.  The role of a regional aggregator is to take connections (large and small) and condense them into long-haul links that will uplink to a backbone or exchange point as a next step.\n\n\nRegional operations teams have similar performance concerns to that of a campus network.  The aggregation point of several networks can be a critical component, and one of the more likely places that a failing piece of equipment or congestion can impact downstream network users.  Monitoring local and remote components (e.g. maintaining active testing between networks) critical.\n\n\nRegional support teams can be likely candidates for assistance on performance problems, but users are reminded to discuss options with their local staff first before engaging with these groups.\n\n\n\n\n3.7.R\nE Backbone\n\n\n\n\nAn R\nE backbone consists of an aggregation of numerous regional providers.  Capacity must reflect the number and expectations of this group of customers, and often is orders of magnitude higher than other networks that are downstream.\n\n\nAs an aggregation point, normally spread over a very large geographical area, traffic flows will be numerous, of mixtures of size and duration, and be destined for diverse destinations domestically and internationally.  Each Point of Presence (PoP) could have a large number of customers integrating, and thus increases the chance of an issue local to this device.\n\n\nAs a service to customers, the R\nE backbone should consider making test instances available to help bisect and debug challenging problems that may cross the domain.  Backbone support teams are also well trained and have knowledge of performance monitoring.  Some providers such as ESnet and Internet2 have dedicated staffs just to support the troubleshooting of network problems for customers.  An end user is encouraged to seek out these resources, as well as those that are local, when debugging a problem impacting scientific work.\n\n\n\n\n3.8.Exchange Point\n\n\n\n\nAn exchange point is normally a location where multiple backbone networks and international transit links (e.g. trans-oceanic links) combine and transit to other domains.  An exchange point is a special case of an aggregation network like a regional in that policies may be different depending on the membership structure.\n\n\nInternational exchange points suffer from the aforementioned problems of traffic aggregation wherein congestion or equipment failure will have a severe impact on all traffic.  Monitoring these devices is crucial, as in other use cases.\n\n\n\n\n4.Actor \n Agent Definitions\n\n\n\n\nThere are many actors involved in the process of network management and debugging.  We will highlight three here, as they represent the most critical members of the support team that OSG has available when problems are discovered.\n\n\n\n\n4.1.End User\n\n\n\n\nThe end user is understood to be the primary user and beneficiary of OSG software to process and operate on scientific data sets.  The sophistication of this end user is assumed to be beginner to average in matters related to system and network administration.  In general we assume they are knowledgeable enough to install and maintain OSG software, and connect devices to the networking infrastructure.\n\n\nThis actor is assumed to be the primary user of the perfSONAR end user tools, packaged in the OSG VDT distribution.  These tools are meant to be run from a system to upstream test machines provided by the campus, regional, or backbone network.\n\n\n\n\n4.2.Local Administrator\n\n\n\n\nThe local administrator can be campus support staff responsible for the health of servers or network devices across the greater campus ecosystem.  Their primary responsibility it to ensure uptime of the network for all users, as well as assist in debugging specific problems caused by performance impacting problems on a local basis.\n\n\nThis actor may not be able to directly address problems on a regional, national, or international basis but can serve as a liaison with individuals within those stewardship organizations.\n\n\n\n\n4.3.Remote Administrator\n\n\n\n\nA remote administrator can be regional, backbone, or exchange point staff responsible for the health of remote networking resources.  It is often the case that these individuals may not be aware of a specific use case between remote campuses, but could answer questions about the current health and status of the network they control.\n\n\nThese individuals are assumed to be knowledgeable about performance tools, and can work with local staff as needed to make test points available to assist in debugging.\n\n\n\n\n5.Local Preparations\n\n\n\n\nA first step to any OSG software installation to support scientific activity is preparation of the local environment.  Given the considerations denoted in the previous sections, we will discuss 3 specific preparation activities:\n\n\n\n\nEnd System Operating System and Protocol Tuning\n\n\nNetwork Architecture Adjustments\n\n\nNetwork Configuration Tuning\n\n\n\n\nEach of these steps is considered to be most relevant to the laboratory local environment, although some should be considered for the campus as well.  It is assumed that the end user actor, with the help of local administrators, can make these changes.\n\n\n\n\n5.1.End System Tuning\n\n\n\n\nComputer systems are similar to automobiles in that its possible to \ntune\n certain internal aspects and achieve higher performance when using the network.  The operating system and associated protocols like TCP make these changes rather simple to implement.  In general there are several options worth considering:\n\n\n\n\nNetwork interface cards have an adjustable size for their packet queues\n\n\nKernel buffers can be increased to support long distance transfers\n\n\nThe TCP congestion control algorithm can be changed\n\n\n\n\nESnet has made a web based resource available to assist with the task of host tuning, it can be found here:\n\n\nhttp://fasterdata.es.net/host-tuning/\n\n\n\n\n5.2.Network Architecture\n\n\n\n\nArchitectural decisions are often involved and will involve the input of local support staff.  In general laboratory architecture should be robust in the following manner:\n\n\n\n\nMultiple uplinks to the campus network to provide capacity and resiliency\n\n\nA limited amount of \nfan in\n (e.g. number of connections) into a given access switch.  It is recommended that as the fan in grows, multiple switches be employed to manage connectivity and congestion\n\n\nElimination of firewalls from the path.  Security can be implemented by host-based firewalls that restrict ports as well as Access Control Lists (ACLs) to white list sites that are communicated with.  Firewall devices have been known to severely impact traffic for bandwidth intensive applications.\n\n\nChoice of device manufacturer that allows for out of band management and monitoring (e.g. SNMP) of devices\n\n\nChoice of device manufacturer that allows for per-interface tuning of memory buffers (vs. that of a shared memory region)\n\n\n\n\nNetwork architectural considerations are far too broad to be represented in a single document for the OSG, and the interested reader is encouraged to read the following resource provided by ESnet:\n\n\nhttp://fasterdata.es.net/science-dmz\n\n\n\n\n5.3.Network Configuration Tuning\n\n\n\n\nMuch like end hosts, network devices have the ability to be \ntuned\n for specific use cases.  This tuning normally centers on enabling or disabling certain features on a router or switch (e.g. policy maps) or adjusting the available memory available to account for a specific use case (e.g. less memory for a video application, more for a throughput intensive tool).\n\n\nAs every manufacturer provides different interfaces to the underlying hardware, we cannot make specific recommendations in this document.  The interested reader is encouraged to read this guide provided by ESnet:\n\n\nhttp://fasterdata.es.net/network-tuning/router-tuning/\n\n\n\n\n6.Measurement Software\n\n\n\n\nThe available span of network measurement software is vast.  A simple web search will reveal 10s of names, some still active and others long dead.  The R\nE community began to standardize on available tools in the later part of the 2000s with an effort to unify measurement tools and infrastructure to support them: perfSONAR.\n\n\nperfSONAR is a framework to simplify end to end network debugging.  It consists of a layer of middleware, designed to sit between the measurement tools and the visualization and analysis that is useful to human users.  A key component of the perfSONAR concept is the pS Performance Toolkit; this completely enclosed operating system packages performance tools and easy to follow GUIs to enable configuration.\n\n\nperfSONAR focuses on several key metrics:\n\n\n\n\nAchievable Bandwidth\n\n\nOne Way Latency\n\n\nRound Trip Latency\n\n\nPacket Loss, Duplication, Out of Orderness\n\n\nInterface Utilization, Errors, Discards\n\n\nLayer 3 Path\n\n\nPath MTU\n\n\n\n\nMany of these metrics are calculated through simple tests that can be run from the command line.  The OSG VDT package contains 3 key measurement tools that will be used as we discuss networking debugging in Section 7:\n\n\n\n\nBWCTL \u2013 A tool for measuring end to end bandwidth availability\n\n\nNDT \u2013 A tool designed to diagnose several different aspects of a host and network\n\n\nOWAMP \u2013 A tool designed for measuring one way delays of packets, as well as loss, duplication, and out of orderness.\n\n\n\n\nThese 3 command line tools, when installed on a compute or storage node, can be used to launch tests to perfSONAR servers located in the greater R\nE networking world, e.g. on the campus, regional, backbone, or exchange point networks.\n\n\n\n\n7.Debugging Process\n\n\n\n\nThe following sections will discuss the process to install, use, and interpret measurement tools in an OSG software environment.  End users are encouraged to try these steps first, but also contact their local support staff at the earliest possible moment.  Involving support staff will ensure that expert eyes are available to assist with problems, and funnel the requests for help to the proper area (e.g. GOC, other networks, etc.).\n\n\n\n\n7.1.Software Installation\n\n\n\n\nClient software can be installed in one of two ways, either though the OSG VDT or via RPMs from the perfSONAR web site.\n\n\n1.\n  1. 7.1.1.OSG Software\n\n\n[INSERT INSTRUCTIONS ON HOW TO INSTALL VDT HERE]\n\n\n1.\n  1. 7.1.2.perfSONAR-PS Software\n\n\nAll perfSONAR software is available through an RPM (Red Hat Package Manager) repository to make for easy installation and updates.  The following steps can be taken to install these tools:\n\n\n\n\nImport the Internet2 Signing Key\n\n\n\n\nThe following command will import the key.\n\n\nsudo rpm --import \nhttp://software.internet2.edu/rpms/RPM-GPG-KEY-Internet2\n\n\n\n\nDownload RPM Software\n\n\n\n\nThe latest version for CentOS 5 and 6 (both x86 and x86_64 architectures) can be found on the the following web site:\n\n\nhttp://software.internet2.edu\n\n\nNote that SL5 andSL6, RHEL 5 and RHEL 6 should work with these builds.  Those using other operating systems are suggested to try source builds that can be found at the same location.\n\n\n\n\nRun Yum Update\n\n\n\n\nThe following command will invoke updates to the yum system, and also prepare the newly installed perfSONAR repository:\n\n\nsudo yum update\n\n\n\n\nSearch for Tools\n\n\n\n\nYum can be searched in the following manner:\n\n\nsudo yum search TOOLNAME\n\n\n\n\nInstall Tools\n\n\n\n\nYum can install tools in a similar fashion, the following command will install the client libraries:\n\n\nsudo yum install owamp-client bwctl-client ndt-client\n\n\nNote that some other tools may be pulled in automatically.  Note that iperf and nuttcp are required for BWCTL to work.\n\n\n\n\n7.2.Tool Selection\n\n\n\n\nDebugging network problems involves running several tools, and gathering results both an end-to-end basis as well as to points in the middle.  Initial tool selection can depend on a couple of factors:\n\n\n\n\nWhat servers are available on the other end, as well as in the middle\n\n\nWhat use case is attempting to be debugged\n\n\nHow sophisticated is the user running the tools\n\n\n\n\nIn general we recommend that users try \n \nall\n available tools, but in a structured and complete fashion before moving on to new tests.  In general the following recommendation can be made regarding tool selection:\n\n\n\n\nPerform NDT client tests to the closest server possible.  Additional tests to other points in the middle as needed.\n\n\nPerform end-to-end BWCTL tests to establish a baseline bandwidth.  Perform a bisected BWCTL test to points on middle networks, testing in areas where performance is bad in favor of where it is good (e.g. narrow down the problem)\n\n\nPerform end-to-end OWAMP tests to establish baseline latency and loss.  Perform a bisected OWAMP test to points on middle networks, testing in areas where performance is bad in favor of where it is good (e.g. narrow down the problem)\n\n\n\n\nThe following are some examples of how to use the tools from the command line:\n\n\n\n\nNDT\n\n\n\n\nNDT uses a command line client called \nweb100clt\n.  There are many options available, but in general you must supply a server name, and some debugging flags to get additional output.  Here is a simple invocation:\n\n\n[user@host ~]$ web100clt -n ndt.chic.net.internet2.edu\n\nTesting network path for configuration and performance problems  --  Using IPv6 address\n\nChecking for Middleboxes . . . . . . . . . . . . . . . . . .  Done\n\nchecking for firewalls . . . . . . . . . . . . . . . . . . .  Done\n\nrunning 10s outbound test (client to server) . . . . .  92.16 Mb/s\n\nrunning 10s inbound test (server to client) . . . . . . 90.63 Mb/s\n\nThe slowest link in the end-to-end path is a 100 Mbps Full duplex Fast Ethernet subnet\n\nInformation: Other network traffic is congesting the link\n\nInformation [S2C]: Packet queuing detected: 15.06% (local buffers)\n\nServer \n#39;ndt.chic.net.internet2.edu\n#39; is not behind a firewall. [Connection to the ephemeral port was successful]\n\nClient is probably behind a firewall. [Connection to the ephemeral port failed]\n\nInformation: Network Middlebox is modifying MSS variable (changed to 1440)\n\nServer IP addresses are preserved End-to-End\n\nClient IP addresses are preserved End-to-End\n\n\n\n\n\nTo get additional data, try adding the \n-ll\n flag, it will produce a more in depth analysis.  NDT is useful as the first step of debugging to gather information about the end host, as well as the basic network configuration.\n\n\n\n\nBWCTL\n\n\n\n\nBWCTL is invoked from the command line with a number of options.  Of these the following are important:\n\n\n-\n  - \n-f\n  - Sets the format, supply either a byte based metric (K, M, G) or a bit based metric (k, m, g).\n  - \n\u2013t\n \u2013 Sets the length of the test in seconds\n  - \n\u2013i\n \u2013 Specifies the reporting interval (e.g. how often instantaneous bandwidth results are available) in seconds\n  - \n\u2013c\n \u2013 Specifics the host that will receive the flow of data, e.g. the \ncatcher\n\n  - \n\u2013s\n \u2013 Specifics the host that will send the flow of data, e.g. the \nsender\n\n\nAn example of invoking BWCTL can be seen below.  In this example we are sending data from the host we are on to another located in Kansas City MO, on the Internet2 network:\n\n\n[user@host ~]$ bwctl -f m -t 10 -i 1 -c nms-rthr.kans.net.internet2.edu\nbwctl: Using tool: iperf\nbwctl: 93 seconds until test results available\nRECEIVER START\nbwctl: exec\\_line: /usr/bin/iperf -B 64.57.16.210 -s -f m -m -p 5011 -t 10 -i 1\nbwctl: start\\_tool: 3568979157.239050\n------------------------------------------------------------\nServer listening on TCP port 5011\nBinding to local address 64.57.16.210\nTCP window size: 0.08 MByte (default)\n------------------------------------------------------------\n[14] local 64.57.16.210 port 5011 connected with 64.57.17.18 port 5011\n[14]  0.0- 1.0 sec    105 MBytes    879 Mbits/sec\n[14]  1.0- 2.0 sec    118 MBytes    990 Mbits/sec\n[14]  2.0- 3.0 sec    118 MBytes    990 Mbits/sec\n[14]  3.0- 4.0 sec    118 MBytes    990 Mbits/sec\n[14]  4.0- 5.0 sec    118 MBytes    990 Mbits/sec\n[14]  5.0- 6.0 sec    118 MBytes    990 Mbits/sec\n[14]  6.0- 7.0 sec    118 MBytes    990 Mbits/sec\n[14]  7.0- 8.0 sec    118 MBytes    990 Mbits/sec\n[14]  8.0- 9.0 sec    118 MBytes    990 Mbits/sec\n[14]  9.0-10.0 sec    118 MBytes    990 Mbits/sec\n[14]  0.0-10.1 sec  1178 MBytes    979 Mbits/sec\n[14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface)\nbwctl: stop\\_exec: 3568979172.016198\nRECEIVER END\n\n\n\n\n\nThis test reveals that over the course of 10 seconds we achieved an average bandwidth of 979Mbps and sent a total of 1178MB of data.  We can reverse the direction of this test in the next example:\n\n\n[user@host ~]$ bwctl -f m -t 10 -i 1 -s nms-rthr.kans.net.internet2.edu\nbwctl: Using tool: iperf\nbwctl: 75 seconds until test results available\nRECEIVER START\nbwctl: exec\\_line: /usr/bin/iperf -B 64.57.17.18 -s -f m -m -p 5011 -t 10 -i 1\nbwctl: start\\_tool: 3568979241.960327\n------------------------------------------------------------\nServer listening on TCP port 5011\nBinding to local address 64.57.17.18\nTCP window size: 16.0 MByte (default)\n------------------------------------------------------------\n[14] local 64.57.17.18 port 5011 connected with 64.57.16.210 port 5011\n[ID] Interval       Transfer     Bandwidth\n[14]  0.0- 1.0 sec   111 MBytes   929 Mbits/sec\n[14]  1.0- 2.0 sec   118 MBytes   990 Mbits/sec\n[14]  2.0- 3.0 sec   118 MBytes   990 Mbits/sec\n[14]  3.0- 4.0 sec   118 MBytes   990 Mbits/sec\n[14]  4.0- 5.0 sec   118 MBytes   990 Mbits/sec\n[14]  5.0- 6.0 sec   118 MBytes   990 Mbits/sec\n[14]  6.0- 7.0 sec   118 MBytes   990 Mbits/sec\n[14]  7.0- 8.0 sec   118 MBytes   990 Mbits/sec\n[14]  8.0- 9.0 sec   118 MBytes   990 Mbits/sec\n[14]  9.0-10.0 sec   118 MBytes   990 Mbits/sec\n[14]  0.0-10.2 sec  1193 MBytes   984 Mbits/sec\n[14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface)\nbwctl: stop\\_exec: 3568979256.889493\nRECEIVER END\n\n\n\n\n\nA similar result is seen in that we achieve near 1Gbps bandwidth (e.g. the hosts are only connected at 1Gbps).\n\n\nBWCTL can (and should) be used to check available bandwidth between servers.  Start first on the long path (e.g. end-to-end) then test to resources in the middle.  Note that BWCTL supports a 3\n\n\nrd\n\n\nmode operation, wherein you can provide options for both the \n-c\n and \n-s\n and perform tests between these two hosts without being physically logged into either:\n\n\n[user@host ~]$ bwctl -f m -t 10 -i 1 -s nms-rthr.kans.net.internet2.edu -c nms-rthr1.hous.net.internet2.edu\nbwctl: Using tool: iperf\nbwctl: 82 seconds until test results available\nRECEIVER START\nbwctl: exec\\_line: /usr/bin/iperf -B 64.57.16.130 -s -f m -m -p 5001 -t 10 -i 1\nbwctl: start\\_tool: 3568979772.344387\n\n------------------------------------------------------------\nServer listening on TCP port 5001\nBinding to local address 64.57.16.130\nTCP window size: 0.08 MByte (default)\n------------------------------------------------------------\n[14] local 64.57.16.130 port 5001 connected with 64.57.16.210 port 5001\n[ID] Interval       Transfer     Bandwidth\n[14]  0.0- 1.0 sec   103 MBytes   861 Mbits/sec\n[14]  1.0- 2.0 sec   118 MBytes   990 Mbits/sec\n[14]  2.0- 3.0 sec   118 MBytes   990 Mbits/sec\n[14]  3.0- 4.0 sec   118 MBytes   990 Mbits/sec\n[14]  4.0- 5.0 sec   118 MBytes   990 Mbits/sec\n[14]  5.0- 6.0 sec   118 MBytes   990 Mbits/sec\n[14]  6.0- 7.0 sec   118 MBytes   990 Mbits/sec\n[14]  7.0- 8.0 sec   118 MBytes   990 Mbits/sec\n[14]  8.0- 9.0 sec   118 MBytes   990 Mbits/sec\n[14]  9.0-10.0 sec   118 MBytes   990 Mbits/sec\n[14]  0.0-10.2 sec  1183 MBytes   977 Mbits/sec\n[14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface)\nbwctl: stop\\_exec: 3568979785.230833\nRECEIVER END\n\n\n\n\n\nBWCTL requires a stable NTP clock to work properly, be sure that NTP is configured before using this tool.\n\n\n\n\nOWAMP\n\n\n\n\nOWAMP is a tool that measures latency, loss, out of orderness, and duplication of packets between a source and a destination.  Note that this test measures each direction \nindependently\n versus that of the traditional round trip tool \nping\n.  Below is an example of a test:\n\n\n[user@host ~]$ owping owamp.wash.net.internet2.edu\nApproximately 12.6 seconds until results available\n--- owping statistics from [eth-1.nms-rlat.newy32aoa.net.internet2.edu]:60455 to [owamp.wash.net.internet2.edu]:47148 ---\nSID:        00160034d4ba4cad4e8c0485546b4ebf\nfirst:        2013-02-04T15:05:18.240\nlast:        2013-02-04T15:05:27.254\n100 sent, 0 lost (0.000%), 0 duplicates\none-way delay min/median/max = 2.02/2.1/2.06 ms, (err=0.218 ms)\none-way jitter = 0 ms (P95-P50)\nHops = 2 (consistently)\nno reordering\n\n--- owping statistics from [owamp.wash.net.internet2.edu]:47149 to [eth-1.nms-rlat.newy32aoa.net.internet2.edu]:33562 ---\nSID:        00170098d4ba4cad8eb45282697d2cc2\nfirst:        2013-02-04T15:05:18.259\nlast:        2013-02-04T15:05:27.175\n100 sent, 0 lost (0.000%), 0 duplicates\none-way delay min/median/max = 3.19/3.3/3.27 ms, (err=0.218 ms)\none-way jitter = 0 ms (P95-P50)\nHops = 2 (consistently)\nno reordering\n\n\n\n\n\nThe results clearly state each direction of operation, and any problems that were found.  As in the BWCTL case the tool is highly reliant on stable NTP numbers, so be sure your server is synchronized against an NTP server.\n\n\nOWAMP is a lightweight test and can be used to show minor amounts of packet loss between hosts.  Perform the test on the full end-to-end path, and then bisect the path by testing to points in the middle.  Often low throughput observed via BWTL will show up as packet loss in OWAMP.\n\n\n\n\n7.3.End-to-end Testing\n\n\n\n\nThe concept of end-to-end testing is required as a first step in debugging network problems.  Via the OSG tools it is possible to use \nclient\n tools as discussed in Section 7.2 to gauge the total end-to-end path.  These client tools can be pointed at a pS Performance Toolkit instance installed on the remote end, or via stand-alone daemon applications started on OSG systems.  In either case, a daemon and client will be needed.\n\n\nThe following procedure should be followed:\n\n\n\n\nNotify local networking staff at each end that are noticing problems, and would like to investigate them.  Note that you can run tests end-to-end, and share them when you are complete.\n\n\nIdentify Servers on both ends (e.g. standalone pS Performance Toolkit instances or starting daemons on OSG servers)\n\n\nIdentify clients on both ends, normally the compute or storage nodes.  Avoid using machines that are not involved in the OSG software process such as laptops.\n\n\nPerform end-to-end testing with:\n\n\nNDT\n\n\nBWCTL\n\n\nOWAMP\n\n\nPerform several tests and always record the results.  It\ns a good idea to run at different times during the day, and note when you ran the tests\n\n\nShare results with local network staff, and open a ticket with the GOC if you feel it is something they can help investigate.\n\n\n\n\nAfter end-to-end testing, and examining the results with local and GOC based professionals, it may be time to embark on a larger debugging exercise with partial path decomposition.\n\n\n\n\n7.4.Partial Path Decomposition\n\n\n\n\nAs we saw in Section 7.4, it is necessary to use all tools in a structured and scripted manner.  Deciding to divide the path is no different.  The following steps should be followed:\n\n\n\n\nUsing a tool like traceroute or tracepath, find the paths between you and the remote host.  If possible validate the path for the reverse direction as well.  It may be possible that these are different.\n\n\nFor one of the networks on the path, usually one in the direct middle (often a backbone network like Internet2, ESnet, or NLR), find a testing host.  If these are not posted on public we pages, send an email to the support teams (e.g. \nrs@internet2.edu\n, or \ntrouble@es.net\n).\n\n\nPerform end-to-middle testing from the source and desgination with:\n\n\nNDT\n\n\nBWCTL\n\n\nOWAMP\n\n\nPerform several tests and always record the results.  It\ns a good idea to run at different times during the day, and note when you ran the tests\n\n\nShare results with local network staff, and open a ticket with the GOC if you feel it is something they can help investigate.\n\n\n\n\nIf the tests show one \nside\n as being better than the other, you can repeat this process by further bisecting the path on the side with the problem.\n\n\n\n\n\n\n7.5.Interpreting Results\n\n\n\n\n\n\nInterpretation of results can be tricky due to the nature of protocols on the network, including TCP.  In general the only symptom that is given off to a problem with TCP is \nlow throughput\n.  The following are some tips on interpreting results:\n\n\n\n\nNDT will denote if your host does not have the proper amount of tuning.  If it doesn\nt, please considering following the host tuning steps discussed in Section 5.1\n\n\nNDT will give the first indication of network problems as well, and may indicate the presence of packet loss, link bottlenecks, or congestion.  Since NDT is based on heuristics these results can turn out to be false positives, but are often worthy of following up on.  In the event of congestion, ask local networking staff to see if there are any heavily utilized links.  If packet loss is an issue, ask to see if any interface errors or discards are present.\n\n\nBWCTL, when used in TCP mode, is only useful at nothing high or low throughput.  This is normally good from a capability standpoint, but it cannot tell you anything else about a serious problem\n\n\nOWAMP is useful for detecting loss.  The theory is that if you notice the loss of small UDP packets produced by OWAMP, the same behavior will be seen in the form of low throughput from a tool like BWCTL.\n\n\nOWAMP can also be used to show asymmetric routing (with the aide of tools like traceroute) or if queuing and congestion are becoming a factor in one direction vs. another.\n\n\nBisecting a path, and being patient, are normally the only ways to narrow down problems.\n\n\n\n\nIn addition to these adages, please consider asking your local staff for assistance when you first notice a problem.  If they are unable to help, consult the resources listed in Section 8.\n\n\n\n\n8.Additional Help\n\n\n\n\nThe following locations can be consulted for more help in debugging network problems:\n\n\n\n\nInternet2 Research Support Center \u2013 \nrs@internet2.edu\n\n\nESnet Trouble Reporting \u2013 \ntrouble@es.net\n\n\nNLR NOC - \nnoc@nlr.net\n\n\n\n\nOSG GOC - \ngoc@opensciencegrid.org\n\n\n\n\n\n\n9.Acknowledgements\n\n\n\n\n\n\nThe authors would like to acknowledge and thank the OSG community for their support and feedback into network performance problems and tools that would be useful for end users.\n\n\nThe perfSONAR-PS community has been invaluable, and the authors would like to thank them for their generous contributions of software, expertise, and time.\n\n\n\n\n10.References\n\n\n\n\nTBD", 
            "title": "OSG Network Debugging Document"
        }, 
        {
            "location": "/network-troubleshooting/osg-debugging-document/#osg-debugging-documentation", 
            "text": "Edited By: J. Zurawski \u2013 Internet2, S. McKee \u2013 University of Michigan  February 4  # th   2013   1.Abstract   Scientific progress relies on intricate systems of computers and the networks that connect them.  Often it is the case that scientific data is gathered via a well defined process, information is digitized, stored, transmitted, and processed by members of large and distributed collaborations.  The Open Science Grid advances science through the concept of distributed computing \u2013 the process for sharing resources through a unified software framework focused on the common tasks of data movement, processing, and analysis.  Networks are an integral part of the distributed computing process.  Similar to the computational and storage resources, it is crucial that all networking components, on the complete end-to-end path, are functional and free of physical and logical flaws.  A rich set of measurement and monitoring tools exists to provide network operations staff and end users a window into the functionality of networks, despite the fact that these actors do not have direct control over the complete path their data may travel.  This document discusses common measurement and monitoring tools available to the OSG community, and strategies to deploy, use, and interpret the results they produce.  The end goal is to give end users more insight into network behavior, and assist local and remote networking staffs as they correct damaging performance problems that will impact the scientific process.   2.Introduction   The process of science is often complicated when viewed as a complete system.  At the core of any project, there is a mechanism to observe or simulate some system, and produce meaningful results that will be interpreted and scrutinized between experimentation runs.   The machinery that surrounds this process can be as benign as simple cameras, or as complex as the Large Hadron Collider and its associated experiments.  Other common components include ways to digitize, store, process, and share the end result of experimentation \u2013 often done using computational systems.  Computation falls into 3 broad classifications, all of which are required to implement the paradigm of scientific computing:   Storage  \u2013 Readable and writable physical medium used for temporary or long term residency of gathered data.  Processing  \u2013 Specially designed hardware and software that iterates over collected data sets looking for pre-defined triggers and results.  Networking  \u2013 Interconnecting hardware and software used to facilitate communication between storage and processing components both on a local, and fully distributed basis.   When fully realized, even a small facility can contribute a great amount of resources to the overall goal of scientific advancement.  In practice it may be the case that a lab consisting of a single researcher can pull data sets from a centralized location, perform carefully selected segments of an entire set of analysis that is required, and return any relevant results as they are discovered.  When used in an inductive fashion, one can imagine the overall throughout that a VO such as the LHC project is able to attain through 100s of distributed facilities and 1000s of collaborating researchers.  Complexity is present as we travel down the individual technology items in the above scenario.  Often it is the case that ideal performance is hard to attain due to the intertwined nature of the mechanisms involved.  For example, data must be written and read from physical medium.  Often this step is slower due to the mechanical nature of the process, and struggles to keep up with faster technologies such as processing or transmission on network infrastructure.  Equally, it may be the case that a flaw in the network infrastructure, such as a failing component, can introduce data loss that must be compensated for through retransmission.  Retransmission implies additional work for storage and processing components that must waste resources to overcome a fixable, but often unnoticed, problem.  Network performance monitoring is a relatively unseen, but still extremely necessary, practice.  This statement is true due to the nature of network use through application software and communication protocols.  Application developers wish to unburden the user with details about  how  data may be moved between facilities.  Care is taken to design applications in such a way that the user is simply presented with options related to a source and destination only, and little or no insight into the path taken or the current conditions that may be present.  The aforementioned situation where a failing component institutes data loss results in only one symptom to the end user: lower than expected throughput.  Many users may not notice, or have become complacent, with low performance situations.  Some may write this off as  the network is slow , or perhaps will not notice at all due to experiences with home connections that are often 2 orders of magnitude slower than what is possible in a typical academic environment.  Software exists to monitor network performance in many different ways.  For example it is possible take a measure of network throughput, and simulate the behavior of a file transfer application.  It is also possible to observe network stability (e.g. jitter) over time to simulate video or audio transmission.  These basic observations are powerful when used both in a local environment, as well as on an end-to-end basis.  In either case, software must be deployed and available to the community on points of interest: specifically on the local and shared network infrastructure distributed around the world.  perfSONAR is a software framework that simplifies network debugging activities by making it easier to deploy measurement tools, and facilities the sharing of results.  It is currently deployed on many communal networking resources in the R E community, including backbones, regional networks, campuses, and individual laboratories.  Once perfSONAR is deployed, it becomes possible to troubleshoot situations that result in low throughout for the end user in a straightforward manner.  It is important to note that when something like this occurs, it is not the sole responsibility of the end user to debug and solve a networking problem; rather it is their duty to report the problem and provide as much information as possible to local or remote network staffs so they may learn about the issue, and interpret the results so as to lead to quick and efficient problem resolution.  Locating this staff may be challenging, but many organizations maintain a dedicated Network Operations Center (NOC) whose staff are ready to accept trouble reports and act in an appropriate manner.    Section 8 details locations you may turn to for additional support.  This document will expand upon these topics in the remaining sections, and conclude with information where additional resources beyond a simple introduction to these topics can be found.   3.The Scientific Networking Process   There is a rich ecosystem of components available for monitoring and managing the scientific networking process.  This myriad of hardware and software must work together to complete the overall goal of interpreting gathered or simulated observations.  Each component we will discuss has the ability to be installed, operated, and maintained in different ways.  Individual brands or versions are not important, but the overall idea of each will be explored.   3.1.Hosts   Server or  host  hardware and software can be used in many different ways.  Often it is the case that these components serve as computational resources for processing data, or provide access to underlying data stored on physical media.  It may also be the case that software designed to  glue  components of a framework together (e.g. processing mail, authenticating users, providing mappings between names and addresses) is installed on a dedicated or shared host resource.  Hosts must contain an operating system: software designed to control and maintain the underlying hardware such as storage media, network interface cards, processors, and other peripheral devices.  Operating system hardware can vary in functionality; completely interactive systems such as those found on laptops can be more pleasing for a human user vs. that of a no frills batch processing system designed to only iterate over scientific data.  The choice of operating system will vary from use case to use case.  The footing provided by the hardware and operating system serves as the basis for the remainder of the components in this discussion.   3.2.Protocols   Protocols are software algorithms implemented on hosts and networking components, and are used to facilitate communication strategies.  Protocols are constructed in a  layered  fashion, and are designed to handle certain aspects of the overall communication plan.  For instance a protocol may be used to communicate between two network devices, and may involve the use of different patterns of electrical or optical signal.  On top of this basic system of signals we may construct a different protocol that is focused on communication between end hosts, and is able to break up the notion of a user s files into small chunks so they can be sent reliably end-to-end.  Protocols evolve with the underlying technology, and often can be tuned to specific use cases.  One such protocol, Transmission Control Protocol (TCP), is widely used in applications that many users are familiar with such as web browsing, mail transfer, and file exchange.  Early incarnations of TCP were designed for the networks of the 1980s; slower, less reliable, constructions than what is present in the networks of the 2010s.  As such TCP must be instructed, via configuration on a host or network devices operating environment,  that it should behave in a different and more efficient manner.  With the protocol in place, we can now begin to discuss applications that are able to use the network to communicate in an automated fashion.   3.3.Applications   Applications are specific use cases, programmed as software, and made available to end users via a host s operating environment.  There are numerous applications we use on a daily basis \u2013 web browsers to fetch remote content, word processing applications to type papers, mail and instant messaging clients to exchange information in near real time.  Scientific applications normally focus on performing a single task (e.g. end-to-end data movement, visualization, data transformation, data analysis) on either a local or distributed basis.  In the case of distribution, it becomes necessary to interact with the underlying network through the use of a protocol.  File transfer is a specialized application that takes is interested in either sending a local file to a remote location, or retrieving remote data to bring locally.  In either case the application must broker with a protocol, such as TCP, that is available on both ends of the transfer.  Through a series of API calls information is segmented into transmission chunks and sent reliably though the communication medium.  Most of this is handled transparently from the user s perspective, and as such they are not given much in the way of feedback beyond a pass or failure, and some notion of how long it took.  Understanding more about the network can be enlightening exercise for users who are unaware of the complexity and span of components that are required for operation.  This will be discussed in the remaining sections.   3.4.Lab Local Area Network   The first step in the networking tree is often the interconnections between components local to the user.  This may consist of the storage and processing nodes in a single rack or data center used for scientific processing, connected via technology consistent with the tasks they are performing.  Cluster nodes may use a high speed interconnect such as InfiniBand; servers may also just use typical Ethernet at 1Gbps or 10Gbps.  In either case, there will be dedicated network equipment with the task of aggregating and controlling traffic flow to the local devices, and serving as an uplink to the next network in the chain (the campus).   Monitoring and management of this local environment is a good idea, either through passive means such as using the SNMP system, or active tests that check the health of transfers on a local basis.   3.5.Campus Local Area Network   The first hop beyond a laboratory environment is a network maintained by campus/local support staff.  It is often the case that this group is maintaining the infrastructure for the use of  all  end users, and as such will design and maintain things to preserve uniform functionality and performance.  Campus networks are an even larger ecosystem of devices given the area they may cover.  It may be the case that the network in the previous section is behind several devices before it has a clean path to the outside world.  It may also be the case that traffic aggregation is extremely high, and congestion becomes a factor during certain parts of the day or times of the year.  These nuances make local performance monitoring crucial for operational soundness.  This group is also the first contact that should be exercised in the event of a network performance problem.  While they may not be able to answer for the status of the entire path, they can escalate the problem to the regional or backbone support staff as needed.   3.6.Regional Network   A regional network provider aggregates the networks of numerous campuses in a state, country, or pre-defined region.  Examples include provider for states of the US (e.g. KanREN), countries (SWITCH, the network of Switzerland), or collaboration between parties without a political boundary (the SOX regional network in the United States).  Regional networks may cover a large geographical area, but often have less equipment than a campus.  The role of a regional aggregator is to take connections (large and small) and condense them into long-haul links that will uplink to a backbone or exchange point as a next step.  Regional operations teams have similar performance concerns to that of a campus network.  The aggregation point of several networks can be a critical component, and one of the more likely places that a failing piece of equipment or congestion can impact downstream network users.  Monitoring local and remote components (e.g. maintaining active testing between networks) critical.  Regional support teams can be likely candidates for assistance on performance problems, but users are reminded to discuss options with their local staff first before engaging with these groups.   3.7.R E Backbone   An R E backbone consists of an aggregation of numerous regional providers.  Capacity must reflect the number and expectations of this group of customers, and often is orders of magnitude higher than other networks that are downstream.  As an aggregation point, normally spread over a very large geographical area, traffic flows will be numerous, of mixtures of size and duration, and be destined for diverse destinations domestically and internationally.  Each Point of Presence (PoP) could have a large number of customers integrating, and thus increases the chance of an issue local to this device.  As a service to customers, the R E backbone should consider making test instances available to help bisect and debug challenging problems that may cross the domain.  Backbone support teams are also well trained and have knowledge of performance monitoring.  Some providers such as ESnet and Internet2 have dedicated staffs just to support the troubleshooting of network problems for customers.  An end user is encouraged to seek out these resources, as well as those that are local, when debugging a problem impacting scientific work.   3.8.Exchange Point   An exchange point is normally a location where multiple backbone networks and international transit links (e.g. trans-oceanic links) combine and transit to other domains.  An exchange point is a special case of an aggregation network like a regional in that policies may be different depending on the membership structure.  International exchange points suffer from the aforementioned problems of traffic aggregation wherein congestion or equipment failure will have a severe impact on all traffic.  Monitoring these devices is crucial, as in other use cases.   4.Actor   Agent Definitions   There are many actors involved in the process of network management and debugging.  We will highlight three here, as they represent the most critical members of the support team that OSG has available when problems are discovered.   4.1.End User   The end user is understood to be the primary user and beneficiary of OSG software to process and operate on scientific data sets.  The sophistication of this end user is assumed to be beginner to average in matters related to system and network administration.  In general we assume they are knowledgeable enough to install and maintain OSG software, and connect devices to the networking infrastructure.  This actor is assumed to be the primary user of the perfSONAR end user tools, packaged in the OSG VDT distribution.  These tools are meant to be run from a system to upstream test machines provided by the campus, regional, or backbone network.   4.2.Local Administrator   The local administrator can be campus support staff responsible for the health of servers or network devices across the greater campus ecosystem.  Their primary responsibility it to ensure uptime of the network for all users, as well as assist in debugging specific problems caused by performance impacting problems on a local basis.  This actor may not be able to directly address problems on a regional, national, or international basis but can serve as a liaison with individuals within those stewardship organizations.   4.3.Remote Administrator   A remote administrator can be regional, backbone, or exchange point staff responsible for the health of remote networking resources.  It is often the case that these individuals may not be aware of a specific use case between remote campuses, but could answer questions about the current health and status of the network they control.  These individuals are assumed to be knowledgeable about performance tools, and can work with local staff as needed to make test points available to assist in debugging.   5.Local Preparations   A first step to any OSG software installation to support scientific activity is preparation of the local environment.  Given the considerations denoted in the previous sections, we will discuss 3 specific preparation activities:   End System Operating System and Protocol Tuning  Network Architecture Adjustments  Network Configuration Tuning   Each of these steps is considered to be most relevant to the laboratory local environment, although some should be considered for the campus as well.  It is assumed that the end user actor, with the help of local administrators, can make these changes.   5.1.End System Tuning   Computer systems are similar to automobiles in that its possible to  tune  certain internal aspects and achieve higher performance when using the network.  The operating system and associated protocols like TCP make these changes rather simple to implement.  In general there are several options worth considering:   Network interface cards have an adjustable size for their packet queues  Kernel buffers can be increased to support long distance transfers  The TCP congestion control algorithm can be changed   ESnet has made a web based resource available to assist with the task of host tuning, it can be found here:  http://fasterdata.es.net/host-tuning/   5.2.Network Architecture   Architectural decisions are often involved and will involve the input of local support staff.  In general laboratory architecture should be robust in the following manner:   Multiple uplinks to the campus network to provide capacity and resiliency  A limited amount of  fan in  (e.g. number of connections) into a given access switch.  It is recommended that as the fan in grows, multiple switches be employed to manage connectivity and congestion  Elimination of firewalls from the path.  Security can be implemented by host-based firewalls that restrict ports as well as Access Control Lists (ACLs) to white list sites that are communicated with.  Firewall devices have been known to severely impact traffic for bandwidth intensive applications.  Choice of device manufacturer that allows for out of band management and monitoring (e.g. SNMP) of devices  Choice of device manufacturer that allows for per-interface tuning of memory buffers (vs. that of a shared memory region)   Network architectural considerations are far too broad to be represented in a single document for the OSG, and the interested reader is encouraged to read the following resource provided by ESnet:  http://fasterdata.es.net/science-dmz   5.3.Network Configuration Tuning   Much like end hosts, network devices have the ability to be  tuned  for specific use cases.  This tuning normally centers on enabling or disabling certain features on a router or switch (e.g. policy maps) or adjusting the available memory available to account for a specific use case (e.g. less memory for a video application, more for a throughput intensive tool).  As every manufacturer provides different interfaces to the underlying hardware, we cannot make specific recommendations in this document.  The interested reader is encouraged to read this guide provided by ESnet:  http://fasterdata.es.net/network-tuning/router-tuning/   6.Measurement Software   The available span of network measurement software is vast.  A simple web search will reveal 10s of names, some still active and others long dead.  The R E community began to standardize on available tools in the later part of the 2000s with an effort to unify measurement tools and infrastructure to support them: perfSONAR.  perfSONAR is a framework to simplify end to end network debugging.  It consists of a layer of middleware, designed to sit between the measurement tools and the visualization and analysis that is useful to human users.  A key component of the perfSONAR concept is the pS Performance Toolkit; this completely enclosed operating system packages performance tools and easy to follow GUIs to enable configuration.  perfSONAR focuses on several key metrics:   Achievable Bandwidth  One Way Latency  Round Trip Latency  Packet Loss, Duplication, Out of Orderness  Interface Utilization, Errors, Discards  Layer 3 Path  Path MTU   Many of these metrics are calculated through simple tests that can be run from the command line.  The OSG VDT package contains 3 key measurement tools that will be used as we discuss networking debugging in Section 7:   BWCTL \u2013 A tool for measuring end to end bandwidth availability  NDT \u2013 A tool designed to diagnose several different aspects of a host and network  OWAMP \u2013 A tool designed for measuring one way delays of packets, as well as loss, duplication, and out of orderness.   These 3 command line tools, when installed on a compute or storage node, can be used to launch tests to perfSONAR servers located in the greater R E networking world, e.g. on the campus, regional, backbone, or exchange point networks.   7.Debugging Process   The following sections will discuss the process to install, use, and interpret measurement tools in an OSG software environment.  End users are encouraged to try these steps first, but also contact their local support staff at the earliest possible moment.  Involving support staff will ensure that expert eyes are available to assist with problems, and funnel the requests for help to the proper area (e.g. GOC, other networks, etc.).   7.1.Software Installation   Client software can be installed in one of two ways, either though the OSG VDT or via RPMs from the perfSONAR web site.  1.\n  1. 7.1.1.OSG Software  [INSERT INSTRUCTIONS ON HOW TO INSTALL VDT HERE]  1.\n  1. 7.1.2.perfSONAR-PS Software  All perfSONAR software is available through an RPM (Red Hat Package Manager) repository to make for easy installation and updates.  The following steps can be taken to install these tools:   Import the Internet2 Signing Key   The following command will import the key.  sudo rpm --import  http://software.internet2.edu/rpms/RPM-GPG-KEY-Internet2   Download RPM Software   The latest version for CentOS 5 and 6 (both x86 and x86_64 architectures) can be found on the the following web site:  http://software.internet2.edu  Note that SL5 andSL6, RHEL 5 and RHEL 6 should work with these builds.  Those using other operating systems are suggested to try source builds that can be found at the same location.   Run Yum Update   The following command will invoke updates to the yum system, and also prepare the newly installed perfSONAR repository:  sudo yum update   Search for Tools   Yum can be searched in the following manner:  sudo yum search TOOLNAME   Install Tools   Yum can install tools in a similar fashion, the following command will install the client libraries:  sudo yum install owamp-client bwctl-client ndt-client  Note that some other tools may be pulled in automatically.  Note that iperf and nuttcp are required for BWCTL to work.   7.2.Tool Selection   Debugging network problems involves running several tools, and gathering results both an end-to-end basis as well as to points in the middle.  Initial tool selection can depend on a couple of factors:   What servers are available on the other end, as well as in the middle  What use case is attempting to be debugged  How sophisticated is the user running the tools   In general we recommend that users try    all  available tools, but in a structured and complete fashion before moving on to new tests.  In general the following recommendation can be made regarding tool selection:   Perform NDT client tests to the closest server possible.  Additional tests to other points in the middle as needed.  Perform end-to-end BWCTL tests to establish a baseline bandwidth.  Perform a bisected BWCTL test to points on middle networks, testing in areas where performance is bad in favor of where it is good (e.g. narrow down the problem)  Perform end-to-end OWAMP tests to establish baseline latency and loss.  Perform a bisected OWAMP test to points on middle networks, testing in areas where performance is bad in favor of where it is good (e.g. narrow down the problem)   The following are some examples of how to use the tools from the command line:   NDT   NDT uses a command line client called  web100clt .  There are many options available, but in general you must supply a server name, and some debugging flags to get additional output.  Here is a simple invocation:  [user@host ~]$ web100clt -n ndt.chic.net.internet2.edu\n\nTesting network path for configuration and performance problems  --  Using IPv6 address\n\nChecking for Middleboxes . . . . . . . . . . . . . . . . . .  Done\n\nchecking for firewalls . . . . . . . . . . . . . . . . . . .  Done\n\nrunning 10s outbound test (client to server) . . . . .  92.16 Mb/s\n\nrunning 10s inbound test (server to client) . . . . . . 90.63 Mb/s\n\nThe slowest link in the end-to-end path is a 100 Mbps Full duplex Fast Ethernet subnet\n\nInformation: Other network traffic is congesting the link\n\nInformation [S2C]: Packet queuing detected: 15.06% (local buffers)\n\nServer  #39;ndt.chic.net.internet2.edu #39; is not behind a firewall. [Connection to the ephemeral port was successful]\n\nClient is probably behind a firewall. [Connection to the ephemeral port failed]\n\nInformation: Network Middlebox is modifying MSS variable (changed to 1440)\n\nServer IP addresses are preserved End-to-End\n\nClient IP addresses are preserved End-to-End  To get additional data, try adding the  -ll  flag, it will produce a more in depth analysis.  NDT is useful as the first step of debugging to gather information about the end host, as well as the basic network configuration.   BWCTL   BWCTL is invoked from the command line with a number of options.  Of these the following are important:  -\n  -  -f   - Sets the format, supply either a byte based metric (K, M, G) or a bit based metric (k, m, g).\n  -  \u2013t  \u2013 Sets the length of the test in seconds\n  -  \u2013i  \u2013 Specifies the reporting interval (e.g. how often instantaneous bandwidth results are available) in seconds\n  -  \u2013c  \u2013 Specifics the host that will receive the flow of data, e.g. the  catcher \n  -  \u2013s  \u2013 Specifics the host that will send the flow of data, e.g. the  sender  An example of invoking BWCTL can be seen below.  In this example we are sending data from the host we are on to another located in Kansas City MO, on the Internet2 network:  [user@host ~]$ bwctl -f m -t 10 -i 1 -c nms-rthr.kans.net.internet2.edu\nbwctl: Using tool: iperf\nbwctl: 93 seconds until test results available\nRECEIVER START\nbwctl: exec\\_line: /usr/bin/iperf -B 64.57.16.210 -s -f m -m -p 5011 -t 10 -i 1\nbwctl: start\\_tool: 3568979157.239050\n------------------------------------------------------------\nServer listening on TCP port 5011\nBinding to local address 64.57.16.210\nTCP window size: 0.08 MByte (default)\n------------------------------------------------------------\n[14] local 64.57.16.210 port 5011 connected with 64.57.17.18 port 5011\n[14]  0.0- 1.0 sec    105 MBytes    879 Mbits/sec\n[14]  1.0- 2.0 sec    118 MBytes    990 Mbits/sec\n[14]  2.0- 3.0 sec    118 MBytes    990 Mbits/sec\n[14]  3.0- 4.0 sec    118 MBytes    990 Mbits/sec\n[14]  4.0- 5.0 sec    118 MBytes    990 Mbits/sec\n[14]  5.0- 6.0 sec    118 MBytes    990 Mbits/sec\n[14]  6.0- 7.0 sec    118 MBytes    990 Mbits/sec\n[14]  7.0- 8.0 sec    118 MBytes    990 Mbits/sec\n[14]  8.0- 9.0 sec    118 MBytes    990 Mbits/sec\n[14]  9.0-10.0 sec    118 MBytes    990 Mbits/sec\n[14]  0.0-10.1 sec  1178 MBytes    979 Mbits/sec\n[14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface)\nbwctl: stop\\_exec: 3568979172.016198\nRECEIVER END  This test reveals that over the course of 10 seconds we achieved an average bandwidth of 979Mbps and sent a total of 1178MB of data.  We can reverse the direction of this test in the next example:  [user@host ~]$ bwctl -f m -t 10 -i 1 -s nms-rthr.kans.net.internet2.edu\nbwctl: Using tool: iperf\nbwctl: 75 seconds until test results available\nRECEIVER START\nbwctl: exec\\_line: /usr/bin/iperf -B 64.57.17.18 -s -f m -m -p 5011 -t 10 -i 1\nbwctl: start\\_tool: 3568979241.960327\n------------------------------------------------------------\nServer listening on TCP port 5011\nBinding to local address 64.57.17.18\nTCP window size: 16.0 MByte (default)\n------------------------------------------------------------\n[14] local 64.57.17.18 port 5011 connected with 64.57.16.210 port 5011\n[ID] Interval       Transfer     Bandwidth\n[14]  0.0- 1.0 sec   111 MBytes   929 Mbits/sec\n[14]  1.0- 2.0 sec   118 MBytes   990 Mbits/sec\n[14]  2.0- 3.0 sec   118 MBytes   990 Mbits/sec\n[14]  3.0- 4.0 sec   118 MBytes   990 Mbits/sec\n[14]  4.0- 5.0 sec   118 MBytes   990 Mbits/sec\n[14]  5.0- 6.0 sec   118 MBytes   990 Mbits/sec\n[14]  6.0- 7.0 sec   118 MBytes   990 Mbits/sec\n[14]  7.0- 8.0 sec   118 MBytes   990 Mbits/sec\n[14]  8.0- 9.0 sec   118 MBytes   990 Mbits/sec\n[14]  9.0-10.0 sec   118 MBytes   990 Mbits/sec\n[14]  0.0-10.2 sec  1193 MBytes   984 Mbits/sec\n[14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface)\nbwctl: stop\\_exec: 3568979256.889493\nRECEIVER END  A similar result is seen in that we achieve near 1Gbps bandwidth (e.g. the hosts are only connected at 1Gbps).  BWCTL can (and should) be used to check available bandwidth between servers.  Start first on the long path (e.g. end-to-end) then test to resources in the middle.  Note that BWCTL supports a 3", 
            "title": "OSG Debugging Documentation"
        }, 
        {
            "location": "/network-troubleshooting/osg-debugging-document/#rd", 
            "text": "mode operation, wherein you can provide options for both the  -c  and  -s  and perform tests between these two hosts without being physically logged into either:  [user@host ~]$ bwctl -f m -t 10 -i 1 -s nms-rthr.kans.net.internet2.edu -c nms-rthr1.hous.net.internet2.edu\nbwctl: Using tool: iperf\nbwctl: 82 seconds until test results available\nRECEIVER START\nbwctl: exec\\_line: /usr/bin/iperf -B 64.57.16.130 -s -f m -m -p 5001 -t 10 -i 1\nbwctl: start\\_tool: 3568979772.344387\n\n------------------------------------------------------------\nServer listening on TCP port 5001\nBinding to local address 64.57.16.130\nTCP window size: 0.08 MByte (default)\n------------------------------------------------------------\n[14] local 64.57.16.130 port 5001 connected with 64.57.16.210 port 5001\n[ID] Interval       Transfer     Bandwidth\n[14]  0.0- 1.0 sec   103 MBytes   861 Mbits/sec\n[14]  1.0- 2.0 sec   118 MBytes   990 Mbits/sec\n[14]  2.0- 3.0 sec   118 MBytes   990 Mbits/sec\n[14]  3.0- 4.0 sec   118 MBytes   990 Mbits/sec\n[14]  4.0- 5.0 sec   118 MBytes   990 Mbits/sec\n[14]  5.0- 6.0 sec   118 MBytes   990 Mbits/sec\n[14]  6.0- 7.0 sec   118 MBytes   990 Mbits/sec\n[14]  7.0- 8.0 sec   118 MBytes   990 Mbits/sec\n[14]  8.0- 9.0 sec   118 MBytes   990 Mbits/sec\n[14]  9.0-10.0 sec   118 MBytes   990 Mbits/sec\n[14]  0.0-10.2 sec  1183 MBytes   977 Mbits/sec\n[14] MSS size 8948 bytes (MTU 8988 bytes, unknown interface)\nbwctl: stop\\_exec: 3568979785.230833\nRECEIVER END  BWCTL requires a stable NTP clock to work properly, be sure that NTP is configured before using this tool.   OWAMP   OWAMP is a tool that measures latency, loss, out of orderness, and duplication of packets between a source and a destination.  Note that this test measures each direction  independently  versus that of the traditional round trip tool  ping .  Below is an example of a test:  [user@host ~]$ owping owamp.wash.net.internet2.edu\nApproximately 12.6 seconds until results available\n--- owping statistics from [eth-1.nms-rlat.newy32aoa.net.internet2.edu]:60455 to [owamp.wash.net.internet2.edu]:47148 ---\nSID:        00160034d4ba4cad4e8c0485546b4ebf\nfirst:        2013-02-04T15:05:18.240\nlast:        2013-02-04T15:05:27.254\n100 sent, 0 lost (0.000%), 0 duplicates\none-way delay min/median/max = 2.02/2.1/2.06 ms, (err=0.218 ms)\none-way jitter = 0 ms (P95-P50)\nHops = 2 (consistently)\nno reordering\n\n--- owping statistics from [owamp.wash.net.internet2.edu]:47149 to [eth-1.nms-rlat.newy32aoa.net.internet2.edu]:33562 ---\nSID:        00170098d4ba4cad8eb45282697d2cc2\nfirst:        2013-02-04T15:05:18.259\nlast:        2013-02-04T15:05:27.175\n100 sent, 0 lost (0.000%), 0 duplicates\none-way delay min/median/max = 3.19/3.3/3.27 ms, (err=0.218 ms)\none-way jitter = 0 ms (P95-P50)\nHops = 2 (consistently)\nno reordering  The results clearly state each direction of operation, and any problems that were found.  As in the BWCTL case the tool is highly reliant on stable NTP numbers, so be sure your server is synchronized against an NTP server.  OWAMP is a lightweight test and can be used to show minor amounts of packet loss between hosts.  Perform the test on the full end-to-end path, and then bisect the path by testing to points in the middle.  Often low throughput observed via BWTL will show up as packet loss in OWAMP.   7.3.End-to-end Testing   The concept of end-to-end testing is required as a first step in debugging network problems.  Via the OSG tools it is possible to use  client  tools as discussed in Section 7.2 to gauge the total end-to-end path.  These client tools can be pointed at a pS Performance Toolkit instance installed on the remote end, or via stand-alone daemon applications started on OSG systems.  In either case, a daemon and client will be needed.  The following procedure should be followed:   Notify local networking staff at each end that are noticing problems, and would like to investigate them.  Note that you can run tests end-to-end, and share them when you are complete.  Identify Servers on both ends (e.g. standalone pS Performance Toolkit instances or starting daemons on OSG servers)  Identify clients on both ends, normally the compute or storage nodes.  Avoid using machines that are not involved in the OSG software process such as laptops.  Perform end-to-end testing with:  NDT  BWCTL  OWAMP  Perform several tests and always record the results.  It s a good idea to run at different times during the day, and note when you ran the tests  Share results with local network staff, and open a ticket with the GOC if you feel it is something they can help investigate.   After end-to-end testing, and examining the results with local and GOC based professionals, it may be time to embark on a larger debugging exercise with partial path decomposition.   7.4.Partial Path Decomposition   As we saw in Section 7.4, it is necessary to use all tools in a structured and scripted manner.  Deciding to divide the path is no different.  The following steps should be followed:   Using a tool like traceroute or tracepath, find the paths between you and the remote host.  If possible validate the path for the reverse direction as well.  It may be possible that these are different.  For one of the networks on the path, usually one in the direct middle (often a backbone network like Internet2, ESnet, or NLR), find a testing host.  If these are not posted on public we pages, send an email to the support teams (e.g.  rs@internet2.edu , or  trouble@es.net ).  Perform end-to-middle testing from the source and desgination with:  NDT  BWCTL  OWAMP  Perform several tests and always record the results.  It s a good idea to run at different times during the day, and note when you ran the tests  Share results with local network staff, and open a ticket with the GOC if you feel it is something they can help investigate.   If the tests show one  side  as being better than the other, you can repeat this process by further bisecting the path on the side with the problem.    7.5.Interpreting Results    Interpretation of results can be tricky due to the nature of protocols on the network, including TCP.  In general the only symptom that is given off to a problem with TCP is  low throughput .  The following are some tips on interpreting results:   NDT will denote if your host does not have the proper amount of tuning.  If it doesn t, please considering following the host tuning steps discussed in Section 5.1  NDT will give the first indication of network problems as well, and may indicate the presence of packet loss, link bottlenecks, or congestion.  Since NDT is based on heuristics these results can turn out to be false positives, but are often worthy of following up on.  In the event of congestion, ask local networking staff to see if there are any heavily utilized links.  If packet loss is an issue, ask to see if any interface errors or discards are present.  BWCTL, when used in TCP mode, is only useful at nothing high or low throughput.  This is normally good from a capability standpoint, but it cannot tell you anything else about a serious problem  OWAMP is useful for detecting loss.  The theory is that if you notice the loss of small UDP packets produced by OWAMP, the same behavior will be seen in the form of low throughput from a tool like BWCTL.  OWAMP can also be used to show asymmetric routing (with the aide of tools like traceroute) or if queuing and congestion are becoming a factor in one direction vs. another.  Bisecting a path, and being patient, are normally the only ways to narrow down problems.   In addition to these adages, please consider asking your local staff for assistance when you first notice a problem.  If they are unable to help, consult the resources listed in Section 8.   8.Additional Help   The following locations can be consulted for more help in debugging network problems:   Internet2 Research Support Center \u2013  rs@internet2.edu  ESnet Trouble Reporting \u2013  trouble@es.net  NLR NOC -  noc@nlr.net   OSG GOC -  goc@opensciencegrid.org    9.Acknowledgements    The authors would like to acknowledge and thank the OSG community for their support and feedback into network performance problems and tools that would be useful for end users.  The perfSONAR-PS community has been invaluable, and the authors would like to thank them for their generous contributions of software, expertise, and time.   10.References   TBD", 
            "title": "rd"
        }, 
        {
            "location": "/perfsonar/psetf/", 
            "text": "Monitoring perfSONAR with ETF (Experiment Testing Framework)\n\n\nA typical perfSONAR deployment has many services that need to function correctly for the the system to work.  As we scale-up to many perfSONAR deployments across many sites it can be\ndifficult to verify everything is working correctly.   \n\n\nWLCG has provided a solution for OSG and WLCG perfSONAR deployments:  perfSONAR ETF\n\n\nHere we describe psetf, it use-cases and operation.", 
            "title": "Infrastructure Monitoring"
        }, 
        {
            "location": "/perfsonar/psetf/#monitoring-perfsonar-with-etf-experiment-testing-framework", 
            "text": "A typical perfSONAR deployment has many services that need to function correctly for the the system to work.  As we scale-up to many perfSONAR deployments across many sites it can be\ndifficult to verify everything is working correctly.     WLCG has provided a solution for OSG and WLCG perfSONAR deployments:  perfSONAR ETF  Here we describe psetf, it use-cases and operation.", 
            "title": "Monitoring perfSONAR with ETF (Experiment Testing Framework)"
        }, 
        {
            "location": "/osg-network-services/", 
            "text": "OSG Network Services\n\n\nOpen Science Grid is providing a number of network-related services to support its members and collaborators in monitoring, understanding, measuring, diagnosing and managing the networks\nused to support their work.\n\n\nHere we will provide an overview of the OSG network services and where to find more information.", 
            "title": "Network Datastore and Streaming"
        }, 
        {
            "location": "/osg-network-services/#osg-network-services", 
            "text": "Open Science Grid is providing a number of network-related services to support its members and collaborators in monitoring, understanding, measuring, diagnosing and managing the networks\nused to support their work.  Here we will provide an overview of the OSG network services and where to find more information.", 
            "title": "OSG Network Services"
        }, 
        {
            "location": "/osg-network-analytics/", 
            "text": "TBA", 
            "title": "Network Analytics"
        }
    ]
}